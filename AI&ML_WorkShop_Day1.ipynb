{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.Regression Example (Using For Loop VS Vectorization)"
      ],
      "metadata": {
        "id": "WNurS_SRF6Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "# Features (x) and weights (w)\n",
        "x = np.array([9, 5, 30, 1])\n",
        "w = np.array([15, 10, -5, 8])\n",
        "b = 5\n",
        "\n",
        "\n",
        "# For loop method\n",
        "def predict_loop(x, w, b):\n",
        "    y = b\n",
        "    for i in range(len(x)):\n",
        "        y += w[i] * x[i]\n",
        "    return y\n",
        "\n",
        "\n",
        "# Vectorized method\n",
        "def predict_vector(x, w, b):\n",
        "    return np.dot(w, x) + b\n",
        "\n",
        "\n",
        "# Measure execution time\n",
        "start = time.time()\n",
        "loop_result = predict_loop(x, w, b)\n",
        "loop_time = time.time() - start\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "vector_result = predict_vector(x, w, b)\n",
        "vector_time = time.time() - start\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"For Loop Delay: {loop_result} minutes\")\n",
        "print(f\"Vectorized Delay: {vector_result} minutes\")\n",
        "print(f\"For Loop Time: {loop_time:.6f} sec\")\n",
        "print(f\"Vectorized Time: {vector_time:.6f} sec\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWBvfWbiGiqf",
        "outputId": "aba8efc4-16b2-487e-ff0e-b49b7a4528d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Loop Delay: 48 minutes\n",
            "Vectorized Delay: 48 minutes\n",
            "For Loop Time: 0.000067 sec\n",
            "Vectorized Time: 0.000096 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2D Feature Matrix (5 samples, 4 features)\n",
        "X = np.array([\n",
        "    [9, 5, 30, 1],  # Flight 1\n",
        "    [3, 7, 20, 0],  # Flight 2\n",
        "    [8, 2, 25, 1],  # Flight 3\n",
        "    [6, 6, 28, 0],  # Flight 4\n",
        "    [5, 4, 35, 1]   # Flight 5\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Weights (same for all samples)\n",
        "W = np.array([15, 10, -5, 8])  # Each feature’s impact on delay\n",
        "b = 5  # Bias term\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Method 1: Using For Loop\n",
        "def predict_loop(X, W, b):\n",
        "    y = []\n",
        "    for i in range(len(X)):  # Loop over each sample (row)\n",
        "        y_i = b\n",
        "        for j in range(len(W)):  # Loop over each feature (column)\n",
        "            y_i += X[i][j] * W[j]\n",
        "        y.append(y_i)\n",
        "    return np.array(y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Method 2: Using Vectorized NumPy (Matrix Multiplication)\n",
        "def predict_vectorized(X, W, b):\n",
        "    return np.dot(X, W) + b  # Matrix multiplication (dot product) + bias\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Measure execution time\n",
        "start = time.time()\n",
        "delay_loop = predict_loop(X, W, b)\n",
        "loop_time = time.time() - start\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "delay_vector = predict_vectorized(X, W, b)\n",
        "vector_time = time.time() - start\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"For Loop Delays: {delay_loop}\")\n",
        "print(f\"Vectorized Delays: {delay_vector}\")\n",
        "print(f\"For Loop Time: {loop_time:.6f} sec\")\n",
        "print(f\"Vectorized Time: {vector_time:.6f} sec\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5LrM0SIGNOK",
        "outputId": "deaa9b91-e1b1-41c8-d9ab-024cbe9057f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Loop Delays: [ 48  20  28  15 -47]\n",
            "Vectorized Delays: [ 48  20  28  15 -47]\n",
            "For Loop Time: 0.000196 sec\n",
            "Vectorized Time: 0.001194 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Logistic Regression"
      ],
      "metadata": {
        "id": "WfyYv3KRG6HU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the libraries"
      ],
      "metadata": {
        "id": "xaCrCNFDHVUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "YlfixD75HX8C"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the dataset"
      ],
      "metadata": {
        "id": "M3wy-VkJHjoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "metadata": {
        "id": "pGoMErjXHhR5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtH5vAgLIEHJ",
        "outputId": "4ccede65-8617-4cdf-f8a0-b87b1bee4bb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-ifs4bnIKN4",
        "outputId": "e1992d57-8b06-409c-aa89-35a5e8027a31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking care of missing data"
      ],
      "metadata": {
        "id": "zxY-chfGKyU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(X[:, 1:3])\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ],
      "metadata": {
        "id": "iRFlIVtAKzYI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTZqzucHK5jx",
        "outputId": "1245ac28-a89e-4143-d980-a71a12589e3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding categorical data"
      ],
      "metadata": {
        "id": "kSpHYSFpLMQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding the Independent Variable"
      ],
      "metadata": {
        "id": "oEkOhVkcIS8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "X = np.array(ct.fit_transform(X))\n"
      ],
      "metadata": {
        "id": "VRGAe51IINXf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_5paUJ-IVoJ",
        "outputId": "c423a51f-30c9-4039-e79f-3b59fa3a062c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding the Dependent Variable"
      ],
      "metadata": {
        "id": "hPkopSXjLgkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "gOi60apXLkqn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGjykwX_LmXZ",
        "outputId": "92c3a3f8-0b50-4bd4-d4b2-60d903d4e97c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the dataset into the Training set and Test set"
      ],
      "metadata": {
        "id": "3figPTnOIl9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
      ],
      "metadata": {
        "id": "PnRrMg9NIf5K"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EozXOr6KLvZg",
        "outputId": "3130e103-c89d-45ca-aaf3-e5c1efd673b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzjr9WtyMG2d",
        "outputId": "faa0649d-cd9f-4317-e788-2933143309fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY8-8_RoMJon",
        "outputId": "048a5fc3-69c8-4241-e87e-546e8f0b8fcb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INPUeYnCMLsF",
        "outputId": "28330644-b613-47ba-cabc-119378009b58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling"
      ],
      "metadata": {
        "id": "ZvCiXRzkMOXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = StandardScaler()\n",
        "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
        "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
      ],
      "metadata": {
        "id": "KZGNX1liMPxj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HveK0q78MWJf",
        "outputId": "898d9cc8-4bab-4b14-b127-9db521adc023"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYd_ps6rMXuN",
        "outputId": "79e49e50-c201-4c3f-8707-4013c60f35f7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Logistic Regression model on the Training set"
      ],
      "metadata": {
        "id": "DfVayBrzItu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "CT9ZsAMhIokZ",
        "outputId": "8e9410a8-cbd7-4ce1-ebfe-b89aac4f5801"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting the Test set results"
      ],
      "metadata": {
        "id": "f-axByxhIzde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "np.set_printoptions(precision=2)\n",
        "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzmKnkwHIwTf",
        "outputId": "8823ac41-08ac-4d0e-8133-72c065095813"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0]\n",
            " [1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Metrics"
      ],
      "metadata": {
        "id": "r_HpQg6KOaIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "print(\"\\nClassification Report:\\n\", class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxVRScNwJLt1",
        "outputId": "9dd8b8bf-04ef-46ba-c579-e3a0bda94d66"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.50\n",
            "\n",
            "Confusion Matrix:\n",
            " [[0 1]\n",
            " [0 1]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualising the Logistic Regression results"
      ],
      "metadata": {
        "id": "dcIzvQ7-Ox8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Actual vs Predicted Values\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(range(len(y_test)), y_test, color='blue', label='Actual Values', alpha=0.6)\n",
        "plt.scatter(range(len(y_pred)), y_pred, color='red', label='Predicted Values', alpha=0.6)\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Class (0 or 1)\")\n",
        "plt.title(\"Actual vs Predicted Values (Logistic Regression)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "xaAbJ9L1OJzw",
        "outputId": "ee0de506-2f63-4488-aba3-86e8e424bd6b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAIjCAYAAAAQgZNYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAXvlJREFUeJzt3Xd4FNX+x/HPpiekEQMJJSSQUDW0IFyQIooGKYoNROlFbIBGVLhKV7CBiKCoVBWlicoFBBFFqhfpKqEEQhOI1IQekpzfH/llL0sKSUhh5P16nn3izpyZ+e7OBD85e+aszRhjBAAAAFiQU3EXAAAAAOQXYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYoBjabTcOGDSvuMordnXfeqTvvvNP+fN++fbLZbJo+fXqx1XS1q2ssKt26dVNYWFiRHzcv5syZo4CAAJ09e7ZY61ixYoVsNptWrFhRIPubPn26bDab9u3bVyD7gzRs2DDZbLZCPcaSJUvk7e2tY8eOFepxcOMhzMLyPvzwQ9lsNjVo0CDf+zh8+LCGDRumLVu2FFxhN7iMAJDxcHV1VaVKldSlSxft3bu3uMvLk7Vr12rYsGE6ffp0kR9706ZNstlseu2117Jts3v3btlsNsXExBRhZYUrNTVVQ4cOVd++feXt7W1fHhYWpjZt2hRjZbk3atQoffvtt4V6jIxgnPFwcXFRuXLl1K1bN/3111+FeuybTcuWLRUREaHRo0cXdykoYoRZWN7MmTMVFham9evXKy4uLl/7OHz4sIYPH35ThdkM/fr10+eff65PPvlErVu31uzZs3X77bfr8OHDRV5LaGioLly4oM6dO+dpu7Vr12r48OHFEmbr1q2ratWq6auvvsq2zZdffilJ6tSpU1GVVej+85//aOfOnXryySeLuxQ1bdpUFy5cUNOmTfO0XXZhtnPnzrpw4YJCQ0MLqEJpxIgR+vzzzzVp0iTdd999+uKLL9SsWTNdvHixwI5xI3vttdd04cKFQj9Onz599PHHH+vMmTOFfizcOAizsLT4+HitXbtWY8eOValSpTRz5sziLslymjRpok6dOql79+764IMP9O677+rkyZOaMWNGttucO3euUGqx2Wzy8PCQs7Nzoey/sDzxxBPau3evfv311yzXf/XVV6pWrZrq1q1bxJUVnmnTpumOO+5QuXLlirsUOTk5ycPDQ05OBfO/NGdnZ3l4eBTox+L33XefOnXqpF69emny5MkaMGCA9uzZowULFhTYMXLDGFMkofJqLi4u8vDwKPTjPPzww7p06ZLmzp1b6MfCjYMwC0ubOXOmSpYsqdatW+uRRx7JNsyePn1aL7zwgsLCwuTu7q7y5curS5cuOn78uFasWKHbb79dktS9e3f7x4EZ4zbDwsLUrVu3TPu8eixlcnKyhgwZoqioKPn5+alEiRJq0qSJfv755zy/roSEBLm4uGj48OGZ1u3cuVM2m00TJkyQJF2+fFnDhw9X5cqV5eHhoVtuuUWNGzfWsmXL8nxcSbrrrrskpf+hIP1vrNv27dv1+OOPq2TJkmrcuLG9/RdffKGoqCh5enoqICBAjz32mA4ePJhpv5988onCw8Pl6emp+vXra9WqVZnaZDdmdseOHWrfvr1KlSolT09PVa1aVa+++qq9vpdeekmSVLFiRfv5u3K8Y0HWmJUnnnhC0v96YK+0ceNG7dy5097mu+++U+vWrVW2bFm5u7srPDxcI0eOVGpqao7HyG5caE7v2SOPPKKAgAB5eHioXr16mYJTfq+dixcvasmSJWrRokWO7bKTkpKikSNHKjw8XO7u7goLC9O///1vXbp0yaFdWlqahg0bprJly8rLy0vNmzfX9u3bM/1OZvXe7N69Ww8//LCCg4Pl4eGh8uXL67HHHlNiYqKk9D+czp07pxkzZtivmYx9Zjdm9vvvv1ezZs3k4+MjX19f3X777Vme89xo0qSJJGnPnj0Oy3Nz3iRp27ZtatasmTw9PVW+fHm9/vrrmjZtWqa6M4Z9LF26VPXq1ZOnp6c+/vhjSen/Lj7//PMKCQmRu7u7IiIi9NZbbyktLc3hWLNmzVJUVJT9dUdGRur999+3r8/NdZTVmNncXgcZr2H16tWqX7++PDw8VKlSJX322WeZ3pfSpUurZs2a+u6773J6+/EP41LcBQDXY+bMmXrooYfk5uamjh076qOPPtJvv/1mD6eSdPbsWTVp0kSxsbHq0aOH6tatq+PHj2vBggU6dOiQqlevrhEjRmjIkCF68skn7f+TadSoUZ5qSUpK0uTJk9WxY0f17t1bZ86c0ZQpUxQdHa3169erdu3aud5XUFCQmjVrpjlz5mjo0KEO62bPni1nZ2c9+uijktL/JzF69Gj16tVL9evXV1JSkjZs2KBNmzbpnnvuydNrkP73P9dbbrnFYfmjjz6qypUra9SoUTLGSJLeeOMNDR48WO3bt1evXr107NgxffDBB2ratKk2b94sf39/SdKUKVPUp08fNWrUSM8//7z27t2r+++/XwEBAQoJCcmxnm3btqlJkyZydXXVk08+qbCwMO3Zs0f/+c9/9MYbb+ihhx7Srl279NVXX+m9995TYGCgJKlUqVJFVmPFihXVqFEjzZkzR++9955Dz3JG2Hn88cclpQclb29vxcTEyNvbWz/99JOGDBmipKQkvfPOO9c6Pbny559/2ntNBw4cqBIlSmjOnDlq166dvv76az344IOS8n/tbNy4UcnJyfnuae7Vq5dmzJihRx55RC+++KL++9//avTo0YqNjdU333xjbzdo0CC9/fbbatu2raKjo7V161ZFR0df86P55ORkRUdH69KlS+rbt6+Cg4P1119/aeHChTp9+rT8/Pz0+eef2193xlCJ8PDwbPc5ffp09ejRQ7feeqsGDRokf39/bd68WUuWLLGf27zICJwlS5a0L8vtefvrr7/UvHlz2Ww2DRo0SCVKlNDkyZPl7u6e5bF27typjh07qk+fPurdu7eqVq2q8+fPq1mzZvrrr7/Up08fVahQQWvXrtWgQYN05MgRjRs3TpK0bNkydezYUXfffbfeeustSVJsbKzWrFmj/v37S8r/dZTb60CS4uLi9Mgjj6hnz57q2rWrpk6dqm7duikqKkq33nqrQ9uoqKhCHwuNG4wBLGrDhg1Gklm2bJkxxpi0tDRTvnx5079/f4d2Q4YMMZLM/PnzM+0jLS3NGGPMb7/9ZiSZadOmZWoTGhpqunbtmml5s2bNTLNmzezPU1JSzKVLlxzanDp1ygQFBZkePXo4LJdkhg4dmuPr+/jjj40k8/vvvzssr1Gjhrnrrrvsz2vVqmVat26d476y8vPPPxtJZurUqebYsWPm8OHDZtGiRSYsLMzYbDbz22+/GWOMGTp0qJFkOnbs6LD9vn37jLOzs3njjTcclv/+++/GxcXFvjw5OdmULl3a1K5d2+H9+eSTT4wkh/cwPj4+03lo2rSp8fHxMfv373c4Tsa5M8aYd955x0gy8fHxhV5jdiZOnGgkmaVLl9qXpaammnLlypmGDRval50/fz7Ttn369DFeXl7m4sWL9mVdu3Y1oaGh9ucZ5+vnn3922Dar9+zuu+82kZGRDvtLS0szjRo1MpUrV7Yvy++1M3ny5CyvTWPSf19y2ueWLVuMJNOrVy+H5QMGDDCSzE8//WSMMebo0aPGxcXFtGvXzqHdsGHDjCSH38mr35vNmzcbSWbu3Lk5vo4SJUpk+bs9bdo0h+vp9OnTxsfHxzRo0MBcuHDBoe2V12FWMvb1448/mmPHjpmDBw+aefPmmVKlShl3d3dz8OBBe9vcnre+ffsam81mNm/ebF924sQJExAQkOn3IDQ01EgyS5Yscahr5MiRpkSJEmbXrl0OywcOHGicnZ3NgQMHjDHG9O/f3/j6+pqUlJRsX2NurqOMf0cy5PY6uPI1rFy50r7s77//Nu7u7ubFF1/MdKxRo0YZSSYhISHHmvDPwTADWNbMmTMVFBSk5s2bS0r/2LBDhw6aNWuWw0e2X3/9tWrVqmXv1bhSQY6Jc3Z2lpubm6T0j0dPnjyplJQU1atXT5s2bcrz/h566CG5uLho9uzZ9mV//PGHtm/frg4dOtiX+fv7688//9Tu3bvzVXePHj1UqlQplS1bVq1bt7Z/9FqvXj2Hdk899ZTD8/nz5ystLU3t27fX8ePH7Y/g4GBVrlzZPrxiw4YN+vvvv/XUU0/Z3x8pfeopPz+/HGs7duyYVq5cqR49eqhChQoO63Jz7oqixgwdOnSQq6urw8fOv/zyi/766y/7EANJ8vT0tP/3mTNndPz4cTVp0kTnz5/Xjh07cnWsnJw8eVI//fST2rdvb9//8ePHdeLECUVHR2v37t32u+jze+2cOHFCkmOvYm4tXrxYkjLN7PDiiy9KkhYtWiRJWr58uVJSUvTMM884tOvbt+81j5FxzpYuXarz58/nucarLVu2TGfOnNHAgQMzjfvM7b8hLVq0UKlSpRQSEqJHHnlEJUqU0IIFC1S+fHlJeTtvS5YsUcOGDR0+7QkICHC4zq5UsWJFRUdHOyybO3eumjRpopIlSzr8brRo0UKpqalauXKlpPRr5Ny5czkOPcnPdZTb6yBDjRo17J+aSemfvFStWjXLmVcyrsvjx4/nuh5YG2EWlpSamqpZs2apefPmio+PV1xcnOLi4tSgQQMlJCRo+fLl9rZ79uzRbbfdViR1zZgxQzVr1rSPGytVqpQWLVpkH6eXF4GBgbr77rs1Z84c+7LZs2fLxcVFDz30kH3ZiBEjdPr0aVWpUkWRkZF66aWXtG3btlwfZ8iQIVq2bJl++uknbdu2TYcPH85yNoGKFSs6PN+9e7eMMapcubJKlSrl8IiNjdXff/8tSdq/f78kqXLlyg7bZ0wFlpOM/1Hl9/wVRY0ZbrnlFkVHR+ubb76xfwz+5ZdfysXFRe3bt7e3+/PPP/Xggw/Kz89Pvr6+KlWqlH2Wg/xcJ1eLi4uTMUaDBw/O9JozhqxkvO7rvXbM/w83yYv9+/fLyclJERERDsuDg4Pl7+9vPxcZP69uFxAQcM0QXbFiRcXExGjy5MkKDAxUdHS0Jk6cmO/3N2PozfX8OzJx4kQtW7ZM8+bNU6tWrXT8+HGHYQF5OW/79+/P9L5Imd+rDFf/7krpvxtLlizJdKyMcdAZx3rmmWdUpUoV3XfffSpfvrx69OihJUuWOOwrP9dRbq+DDFf/MSulh9ZTp05lWp5xXRb2vLa4cTBmFpb0008/6ciRI5o1a5ZmzZqVaf3MmTN17733FsixsvsHMTU11WFs5BdffKFu3bqpXbt2eumll1S6dGk5Oztr9OjRmW7yyK3HHntM3bt315YtW1S7dm3NmTNHd999t31cqJQ+LdGePXv03Xff6YcfftDkyZP13nvvadKkSerVq9c1jxEZGZmrG3mu7FGU0nufbTabvv/++yxnH7hy7tHiUtQ1durUSQsXLtTChQt1//336+uvv9a9995rH797+vRpNWvWTL6+vhoxYoTCw8Pl4eGhTZs26ZVXXsl0482VcroOr5SxjwEDBmTqjcuQESDye+1kjKc+deqUvWcxrwo7aIwZM0bdunWzv7Z+/fpp9OjR+vXXX/Nd8/WoX7++/dOOdu3aqXHjxnr88ce1c+dOeXt75+m85dXVv7tS+nVyzz336OWXX85ymypVqkhKv6Fqy5YtWrp0qb7//nt9//33mjZtmrp06WKf8eR6/g3K7XWQ3QwnWf1BlRFwr/x3Ev9shFlY0syZM1W6dGlNnDgx07r58+frm2++0aRJk+Tp6anw8HD98ccfOe4vp39QS5YsmeX8pfv373fotZs3b54qVaqk+fPnO+zv6hu48qJdu3bq06ePfajBrl27NGjQoEztAgIC1L17d3Xv3l1nz55V06ZNNWzYsFyF2fwKDw+XMUYVK1a0/48vKxlzde7evds+U4KUfgd0fHy8atWqle22Ge9vfs9fUdR4pfvvv18+Pj768ssv5erqqlOnTjl89LtixQqdOHFC8+fPd5gTNWPmiJxk9EZefS1e3YOV8Z65urrm6o+U/Fw71apVs9cdGRl5zWNcKTQ0VGlpadq9e7eqV69uX56QkKDTp0/bz0XGz7i4OIeexRMnTmTZG5eVyMhIRUZG6rXXXtPatWt1xx13aNKkSXr99dcl5T5IZdwY9scff+Q7UF4p44/c5s2ba8KECRo4cGCezltoaGiWc2rnZZ7t8PBwnT17NlfXiJubm9q2bau2bdsqLS1NzzzzjD7++GMNHjzY/n7k9TrK7XWQH/Hx8QoMDLT/EYl/PoYZwHIuXLig+fPnq02bNnrkkUcyPZ577jmdOXPGPp3Nww8/rK1bt2a6O1b631/1JUqUkJQ5KEjp/+j/+uuvSk5Oti9buHBhpqmdMnoOruwp+O9//6t169bl+7X6+/srOjpac+bM0axZs+Tm5qZ27do5tMkYv5jB29tbERERmaa3KWgPPfSQnJ2dNXz48Ey9I8YYe1316tVTqVKlNGnSJIf3cPr06df8koNSpUqpadOmmjp1qg4cOJDpGBmyO39FUeOVPD099eCDD2rx4sX66KOPVKJECT3wwAP29VldI8nJyfrwww+vue/Q0FA5OzvbxzJmuHrb0qVL684779THH3+sI0eOZNrPlV/1md9rJyoqSm5ubtqwYcM1675aq1atJMl+t3yGsWPHSpJat24tSbr77rvl4uKijz76yKFdxpR0OUlKSlJKSorDssjISDk5OTm8thIlSuTq/N57773y8fHR6NGjM82kkJ+hFlL61H7169fXuHHjdPHixTydt+joaK1bt87hS15OnjyZp3m227dvr3Xr1mnp0qWZ1p0+fdr+/l19jTg5OalmzZqSZH8v83Md5fY6yI+NGzeqYcOG+d4e1kPPLCxnwYIFOnPmjO6///4s1//rX/+yf4FChw4d9NJLL2nevHl69NFH1aNHD0VFRenkyZNasGCBJk2apFq1aik8PFz+/v6aNGmSfHx8VKJECTVo0EAVK1ZUr169NG/ePLVs2VLt27fXnj179MUXX2SaxqdNmzaaP3++HnzwQbVu3Vrx8fGaNGmSatSocV3fXd+hQwd16tRJH374oaKjo+1TSWWoUaOG7rzzTkVFRSkgIEAbNmzQvHnz9Nxzz+X7mLkRHh6u119/XYMGDdK+ffvUrl07+fj4KD4+Xt98842efPJJDRgwQK6urnr99dfVp08f3XXXXerQoYPi4+M1bdq0XI1HHT9+vBo3bqy6devqySefVMWKFbVv3z4tWrTI/j/zqKgoSdKrr76qxx57TK6urmrbtm2R1XilTp066bPPPtPSpUv1xBNP2IO2lD7dW8mSJdW1a1f169dPNptNn3/+ea4CkZ+fnx599FF98MEHstlsCg8P18KFC+1jG680ceJENW7cWJGRkerdu7cqVaqkhIQErVu3TocOHdLWrVsl5f/a8fDw0L333qsff/xRI0aMyLQ+Li7O3vt5pTp16qh169bq2rWrPvnkE/uwi/Xr12vGjBlq166d/YbOoKAg9e/fX2PGjNH999+vli1bauvWrfr+++8VGBiYY6/qTz/9pOeee06PPvqoqlSpopSUFH3++edydnbWww8/bG8XFRWlH3/8UWPHjlXZsmVVsWLFLL8W29fXV++995569eql22+/3T7f8tatW3X+/Pkcv2AkJy+99JIeffRRTZ8+XU899VSuz9vLL7+sL774Qvfcc4/69u1rn5qrQoUKOnnyZK56nF966SUtWLBAbdq0sU9xde7cOf3++++aN2+e9u3bp8DAQPXq1UsnT57UXXfdpfLly2v//v364IMPVLt2bXuPan6uo1q1auXqOsirv//+W9u2bdOzzz6br+1hUUU8ewJw3dq2bWs8PDzMuXPnsm3TrVs34+rqao4fP26MSZ+25rnnnjPlypUzbm5upnz58qZr16729cYY891335kaNWoYFxeXTFMdjRkzxpQrV864u7ubO+64w2zYsCHT1FxpaWlm1KhRJjQ01Li7u5s6deqYhQsXZppiyZjcTc2VISkpyXh6ehpJ5osvvsi0/vXXXzf169c3/v7+xtPT01SrVs288cYbJjk5Ocf9ZkxndK3pizKm1Dl27FiW67/++mvTuHFjU6JECVOiRAlTrVo18+yzz5qdO3c6tPvwww9NxYoVjbu7u6lXr55ZuXJlpvcwq2mmjDHmjz/+MA8++KDx9/c3Hh4epmrVqmbw4MEObUaOHGnKlStnnJycMk1PVJA1XktKSoopU6aMkWQWL16caf2aNWvMv/71L+Pp6WnKli1rXn75ZbN06dJM025ldd0cO3bMPPzww8bLy8uULFnS9OnTx/zxxx9Zvmd79uwxXbp0McHBwcbV1dWUK1fOtGnTxsybN8/eJr/XjjHGzJ8/39hsNvsUThkyplHK6tGzZ09jjDGXL182w4cPNxUrVjSurq4mJCTEDBo0yGFKqoz3cvDgwSY4ONh4enqau+66y8TGxppbbrnFPPXUU/Z2V0/NtXfvXtOjRw8THh5uPDw8TEBAgGnevLn58ccfHfa/Y8cO07RpU/vvV8Y0XVdPzZVhwYIFplGjRsbT09P4+vqa+vXrm6+++irH9yljXxlT3V0pNTXVhIeHm/DwcPvUV7k5b8akTz/WpEkT4+7ubsqXL29Gjx5txo8fbySZo0ePOpyP7KbNOnPmjBk0aJCJiIgwbm5uJjAw0DRq1Mi8++679mtg3rx55t577zWlS5c2bm5upkKFCqZPnz7myJEj9v3k5jq6emouY3J/HWT3GrL63fzoo4+Ml5eXSUpKyvI145/JZkw+PyMBANy0UlNTVaNGDbVv314jR44ssuOePn1aJUuW1Ouvv27/Fjike/755/Xxxx/r7NmzlvtK6IJSp04d3XnnnXrvvfeKuxQUIcbMAgDyzNnZWSNGjNDEiROvaxhNTi5cuJBpWcYYyyu/SvpmdPV7c+LECX3++edq3LjxTRtklyxZot27d2d5kyz+2eiZBQDckKZPn67p06erVatW8vb21urVq/XVV1/p3nvvzfLGpZtJ7dq1deedd6p69epKSEjQlClTdPjwYS1fvtxhpgzgZsANYACAG1LNmjXl4uKit99+W0lJSfabwrK6uexm06pVK82bN0+ffPKJbDab6tatqylTphBkcVOiZxYAAACWxZhZAAAAWBZhFgAAAJZ1042ZTUtL0+HDh+Xj41Po3w0OAACAvDPG6MyZMypbtqycnHLue73pwuzhw4cVEhJS3GUAAADgGg4ePKjy5cvn2OamC7M+Pj6S0t8cX1/fYq4GAAAAV0tKSlJISIg9t+XkpguzGUMLfH19CbMAAAA3sNwMCeUGMAAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFgWYRYAAACWRZgFAACAZRFmAQAAYFmEWQAAAFiWS3EX8I+WlibFxUmJiZKfnxQRITnx9wMAALCOtJQ07V8epwtHE+UZ7KfQuyPk5HLj5JlirWTlypVq27atypYtK5vNpm+//faa26xYsUJ169aVu7u7IiIiNH369EKvM182b5ZiYqS+faUBA9J/xsSkLwcAALCAHV9t1srbY3Tiib5K7jdAJ57oq5W3x2jHVzdOninWMHvu3DnVqlVLEydOzFX7+Ph4tW7dWs2bN9eWLVv0/PPPq1evXlq6dGkhV5pHmzdLI0ZIGzdKAQFS5crpPzduTF9OoAUAADe4HV9t1ukXR6jkno26VCJASUGVdalEgEru2ajTL464YQKtzRhjirsISbLZbPrmm2/Url27bNu88sorWrRokf744w/7sscee0ynT5/WkiVLcnWcpKQk+fn5KTExUb6+vtdbdmZpaek9sBs3StWrSzbb/9YZI8XGSvXqSWPGMOQAAADckNJS0rTy9hiV3LNRiWUz5xm/w7E6FVFPTdePKZQhB3nJa5ZKU+vWrVOLFi0clkVHR2vdunXZbnPp0iUlJSU5PApVXFx6YC1f3vHES+nPy5eXtm9PbwcAAHAD2r88Tt4HY3W2ZNZ55mzJ8vI+sF37lxd/nrFUmD169KiCgoIclgUFBSkpKUkXLlzIcpvRo0fLz8/P/ggJCSncIhMTpYsXpRIlsl7v5ZW+PjGxcOsAAADIpwtHE+Vy+aJS3bPOM6nuXnK5fFEXjhZ/nrFUmM2PQYMGKTEx0f44ePBg4R7Qz0/y8JDOnct6/fnz6ev9/Aq3DgAAgHzyDPZTiquHnC9lnWecL51XiquHPIOLP89YKswGBwcrISHBYVlCQoJ8fX3l6emZ5Tbu7u7y9fV1eBSqiIj0sbKHDqWPkb2SMenLa9RIbwcAAHADCr07QmdDqsv7VNZ5xvvUIZ2tUEOhdxd/nrFUmG3YsKGWL1/usGzZsmVq2LBhMVWUBScnqWtXKTAwfexsUpKUkpL+MzY2fXmXLtz8BQAAblhOLk4KfrmrLngHyu9wrJzPJ0mpKXI+nyS/w7G64B2o4Je63BDzzRZrBWfPntWWLVu0ZcsWSelTb23ZskUHDhyQlD5EoEuXLvb2Tz31lPbu3auXX35ZO3bs0Icffqg5c+bohRdeKI7ys1enjjRkiBQVJZ08mX6z18mT6bMYDBmSvh4AAOAGVq1jHfmPGaJT4VFyP3dSvglxcj93Uqci6sl/zBBV63hj5JlinZprxYoVat68eablXbt21fTp09WtWzft27dPK1ascNjmhRde0Pbt21W+fHkNHjxY3bp1y/UxC31qrivxDWAAAMDiiuMbwPKS126YeWaLSpGGWQAAAOTZP3aeWQAAAOBKhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZxR5mJ06cqLCwMHl4eKhBgwZav359ju3HjRunqlWrytPTUyEhIXrhhRd08eLFIqoWAAAAN5JiDbOzZ89WTEyMhg4dqk2bNqlWrVqKjo7W33//nWX7L7/8UgMHDtTQoUMVGxurKVOmaPbs2fr3v/9dxJUDAADgRlCsYXbs2LHq3bu3unfvrho1amjSpEny8vLS1KlTs2y/du1a3XHHHXr88ccVFhame++9Vx07drxmby4AAAD+mYotzCYnJ2vjxo1q0aLF/4pxclKLFi20bt26LLdp1KiRNm7caA+ve/fu1eLFi9WqVatsj3Pp0iUlJSU5PAAAAPDP4FJcBz5+/LhSU1MVFBTksDwoKEg7duzIcpvHH39cx48fV+PGjWWMUUpKip566qkchxmMHj1aw4cPL9DaAQAAcGMo9hvA8mLFihUaNWqUPvzwQ23atEnz58/XokWLNHLkyGy3GTRokBITE+2PgwcPFmHFAAAAKEzF1jMbGBgoZ2dnJSQkOCxPSEhQcHBwltsMHjxYnTt3Vq9evSRJkZGROnfunJ588km9+uqrcnLKnM3d3d3l7u5e8C8AAAAAxa7Yembd3NwUFRWl5cuX25elpaVp+fLlatiwYZbbnD9/PlNgdXZ2liQZYwqvWAAAANyQiq1nVpJiYmLUtWtX1atXT/Xr19e4ceN07tw5de/eXZLUpUsXlStXTqNHj5YktW3bVmPHjlWdOnXUoEEDxcXFafDgwWrbtq091AIAAODmUaxhtkOHDjp27JiGDBmio0ePqnbt2lqyZIn9prADBw449MS+9tprstlseu211/TXX3+pVKlSatu2rd54443iegkAAAAoRjZzk30+n5SUJD8/PyUmJsrX17e4ywEAAMBV8pLXLDWbAQAAAHAlwiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALCsYg+zEydOVFhYmDw8PNSgQQOtX78+x/anT5/Ws88+qzJlysjd3V1VqlTR4sWLi6haAAAA3EhcivPgs2fPVkxMjCZNmqQGDRpo3Lhxio6O1s6dO1W6dOlM7ZOTk3XPPfeodOnSmjdvnsqVK6f9+/fL39+/6IsHAABAsbMZY0x+Nrx8+bKOHj2q8+fPq1SpUgoICMjzPho0aKDbb79dEyZMkCSlpaUpJCREffv21cCBAzO1nzRpkt555x3t2LFDrq6u+SlbSUlJ8vPzU2Jionx9ffO1DwAAABSevOS1PA0zOHPmjD766CM1a9ZMvr6+CgsLU/Xq1VWqVCmFhoaqd+/e+u2333K1r+TkZG3cuFEtWrT4XzFOTmrRooXWrVuX5TYLFixQw4YN9eyzzyooKEi33XabRo0apdTU1GyPc+nSJSUlJTk8AAAA8M+Q6zA7duxYhYWFadq0aWrRooW+/fZbbdmyRbt27dK6des0dOhQpaSk6N5771XLli21e/fuHPd3/PhxpaamKigoyGF5UFCQjh49muU2e/fu1bx585SamqrFixdr8ODBGjNmjF5//fVsjzN69Gj5+fnZHyEhIbl9yQAAALjB5XrM7G+//aaVK1fq1ltvzXJ9/fr11aNHD02aNEnTpk3TqlWrVLly5QIrVEofhlC6dGl98skncnZ2VlRUlP766y+98847Gjp0aJbbDBo0SDExMfbnSUlJBFoAAIB/iFyH2a+++ipX7dzd3fXUU09ds11gYKCcnZ2VkJDgsDwhIUHBwcFZblOmTBm5urrK2dnZvqx69eo6evSokpOT5ebmlmU97u7uuaodAAAA1lJsU3O5ubkpKipKy5cvty9LS0vT8uXL1bBhwyy3ueOOOxQXF6e0tDT7sl27dqlMmTJZBlkAAAD8sxVomN2zZ4/uuuuuXLePiYnRp59+qhkzZig2NlZPP/20zp07p+7du0uSunTpokGDBtnbP/300zp58qT69++vXbt2adGiRRo1apSeffbZgnwZAAAAsIgCnWf27Nmz+uWXX3LdvkOHDjp27JiGDBmio0ePqnbt2lqyZIn9prADBw7Iyel/eTskJERLly7VCy+8oJo1a6pcuXLq37+/XnnllYJ8GQAAALCIPM0zO378+BzX//XXX3r33XdznCqruDHPLAAAwI0tL3ktTz2zzz//fI7jU5OTk/OyOwAAAOC65CnMhoaG6q233lL79u2zXL9lyxZFRUUVSGEAAADAteTpBrCoqCht3Lgx2/U2m035/HZcAAAAIM/y1DM7YsQInT9/Ptv1NWrUUHx8/HUXBQAAAORGnsJsjRo1clzv6uqq0NDQ6yoIAAAAyK1i+9IEAAAA4HoRZgEAAGBZhFkAAABYFmEWAAAAlpXnMHv58mXdfffd2r17d2HUAwAAAORansOsq6urtm3bVhi1AAAAAHmSr2EGnTp10pQpUwq6FgAAACBP8jTPbIaUlBRNnTpVP/74o6KiolSiRAmH9WPHji2Q4gAAAICc5CvM/vHHH6pbt64kadeuXQ7rbDbb9VcFAAAA5EK+wuzPP/9c0HUAAAAAeXbdU3MdOnRIhw4dKohaAAAAgDzJV5hNS0vTiBEj5Ofnp9DQUIWGhsrf318jR45UWlpaQdcIAAAAZClfwwxeffVVTZkyRW+++abuuOMOSdLq1as1bNgwXbx4UW+88UaBFgkAAABkxWaMMXndqGzZspo0aZLuv/9+h+XfffednnnmGf31118FVmBBS0pKkp+fnxITE+Xr61vc5QAAAOAqeclr+RpmcPLkSVWrVi3T8mrVqunkyZP52SUAAACQZ/kKs7Vq1dKECRMyLZ8wYYJq1ap13UUBAAAAuZGvMbNvv/22WrdurR9//FENGzaUJK1bt04HDx7U4sWLC7RAAAAAIDv56plt1qyZdu3apQcffFCnT5/W6dOn9dBDD2nnzp1q0qRJQdcIAAAAZClfN4BZGTeAAQAA3NgK/QYwAAAA4EZAmAUAAIBlEWYBAABgWYRZAAAAWFa+puaSpMTERB09elSSFBwcLD8/vwIrCgAAAMiNPPfMTp48WTVq1FBAQIBq1Kjh8N9TpkwpjBoBAACALOWpZ/add97RsGHD1K9fP0VHRysoKEiSlJCQoB9++EH9+/fXqVOnNGDAgEIpFgAAALhSnuaZDQ0N1TvvvKP27dtnuX727Nl66aWXdODAgQIrsKAxzywAAMCNrdDmmf37778VGRmZ7frIyEgdP348L7sEAAAA8i1PYfb222/Xm2++qZSUlEzrUlNT9dZbb+n2228vsOIAAACAnORpzOyECRMUHR2t4OBgNW3a1GHM7MqVK+Xm5qYffvihUAoFAAAArpanMbOSdObMGX3xxRf69ddfHabmatiwoR5//PEbfhwqY2YBAABubHnJa3kOs1ZHmAUAALixFcoNYOfOnctTEXltDwAAAORVrsNsRESE3nzzTR05ciTbNsYYLVu2TPfdd5/Gjx9fIAUCAAAA2cn1DWArVqzQv//9bw0bNky1atVSvXr1VLZsWXl4eOjUqVPavn271q1bJxcXFw0aNEh9+vQpzLoBAACAvI+ZPXDggObOnatVq1Zp//79unDhggIDA1WnTh1FR0frvvvuk7Ozc2HVe90YMwsAAHBj4wawHBBmAQAAbmyF9g1gAAAAwI2EMAsAAADLIswCAADAsgizAAAAsCzCLAAAACwrX2F2yZIlWr16tf35xIkTVbt2bT3++OM6depUgRUHAAAA5CRfYfall15SUlKSJOn333/Xiy++qFatWik+Pl4xMTEFWiAAAACQnVx/A9iV4uPjVaNGDUnS119/rTZt2mjUqFHatGmTWrVqVaAFAgAAANnJV8+sm5ubzp8/L0n68ccfde+990qSAgIC7D22AAAAQGHLV89s48aNFRMTozvuuEPr16/X7NmzJUm7du1S+fLlC7RAAAAAIDv56pmdMGGCXFxcNG/ePH300UcqV66cJOn7779Xy5YtC7RAAAAAIDs2Y4wp7iKKUl6+6xcAAABFLy95LV89s5s2bdLvv/9uf/7dd9+pXbt2+ve//63k5OT87BIAAADIs3yF2T59+mjXrl2SpL179+qxxx6Tl5eX5s6dq5dffrlACwQAAACyk68wu2vXLtWuXVuSNHfuXDVt2lRffvmlpk+frq+//rog6wMAAACyla8wa4xRWlqapPSpuTLmlg0JCdHx48cLrjoAAAAgB/kKs/Xq1dPrr7+uzz//XL/88otat24tKf3LFIKCggq0QAAAACA7+Qqz48aN06ZNm/Tcc8/p1VdfVUREhCRp3rx5atSoUYEWCAAAAGSnQKfmunjxopydneXq6lpQuyxwTM0FAABwY8tLXsvXN4Blx8PDoyB3BwAAAOQoX2E2NTVV7733nubMmaMDBw5kmlv25MmTBVIcAAAAkJN8jZkdPny4xo4dqw4dOigxMVExMTF66KGH5OTkpGHDhhVwiQAAAEDW8hVmZ86cqU8//VQvvviiXFxc1LFjR02ePFlDhgzRr7/+WtA1AgAAAFnKV5g9evSoIiMjJUne3t5KTEyUJLVp00aLFi0quOoAAACAHOQrzJYvX15HjhyRJIWHh+uHH36QJP32229yd3cvuOoAAACAHOQrzD744INavny5JKlv374aPHiwKleurC5duqhHjx4FWiAAAACQnQKZZ3bdunVat26dKleurLZt2xZEXYWGeWYBAABubEU+z2zDhg3VsGHDgtgVAAAAkGu5DrMLFizI9U7vv//+fBUDAAAA5EWuw2y7du1y1c5msyk1NTW/9QAAAAC5luswm5aWVph1AAAAAHmWr9kMCtrEiRMVFhYmDw8PNWjQQOvXr8/VdrNmzZLNZst1rzEAAAD+WfIUZn/66SfVqFFDSUlJmdYlJibq1ltv1cqVK/NUwOzZsxUTE6OhQ4dq06ZNqlWrlqKjo/X333/nuN2+ffs0YMAANWnSJE/HAwAAwD9HnsLsuHHj1Lt37yynSPDz81OfPn303nvv5amAsWPHqnfv3urevbtq1KihSZMmycvLS1OnTs12m9TUVD3xxBMaPny4KlWqlKfjAQAA4J8jT2F269atatmyZbbr7733Xm3cuDHX+0tOTtbGjRvVokWL/xXk5KQWLVpo3bp12W43YsQIlS5dWj179rzmMS5duqSkpCSHBwAAAP4Z8hRmExIS5Orqmu16FxcXHTt2LNf7O378uFJTUxUUFOSwPCgoSEePHs1ym9WrV2vKlCn69NNPc3WM0aNHy8/Pz/4ICQnJdX0AAAC4seUpzJYrV05//PFHtuu3bdumMmXKXHdR2Tlz5ow6d+6sTz/9VIGBgbnaZtCgQUpMTLQ/Dh48WGj1AQAAoGjl6RvAWrVqpcGDB6tly5by8PBwWHfhwgUNHTpUbdq0yfX+AgMD5ezsrISEBIflCQkJCg4OztR+z5492rdvn8NX5mZMGebi4qKdO3cqPDzcYRt3d3e5u7vnuiYAAABYh80YY3LbOCEhQXXr1pWzs7Oee+45Va1aVZK0Y8cOTZw4Uampqdq0aVOmYQM5adCggerXr68PPvhAUno4rVChgp577jkNHDjQoe3FixcVFxfnsOy1117TmTNn9P7776tKlSpyc3PL8Xh5+a5fAAAAFL285LU89cwGBQVp7dq1evrppzVo0CBl5GCbzabo6GhNnDgxT0FWkmJiYtS1a1fVq1dP9evX17hx43Tu3Dl1795dktSlSxeVK1dOo0ePloeHh2677TaH7f39/SUp03IAAAD88+UpzEpSaGioFi9erFOnTikuLk7GGFWuXFklS5bMVwEdOnTQsWPHNGTIEB09elS1a9fWkiVL7KH4wIEDcnK6Ib7bAQAAADeYPA0z+CdgmAEAAMCNLS95jS5PAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWBZhFgAAAJZFmAUAAIBlEWYBAABgWYRZAAAAWNYNEWYnTpyosLAweXh4qEGDBlq/fn22bT/99FM1adJEJUuWVMmSJdWiRYsc2wMAAOCfq9jD7OzZsxUTE6OhQ4dq06ZNqlWrlqKjo/X3339n2X7FihXq2LGjfv75Z61bt04hISG699579ddffxVx5QAAAChuNmOMKc4CGjRooNtvv10TJkyQJKWlpSkkJER9+/bVwIEDr7l9amqqSpYsqQkTJqhLly7XbJ+UlCQ/Pz8lJibK19f3uusHAABAwcpLXivWntnk5GRt3LhRLVq0sC9zcnJSixYttG7dulzt4/z587p8+bICAgKyXH/p0iUlJSU5PAAAAPDPUKxh9vjx40pNTVVQUJDD8qCgIB09ejRX+3jllVdUtmxZh0B8pdGjR8vPz8/+CAkJue66AQAAcGMo9jGz1+PNN9/UrFmz9M0338jDwyPLNoMGDVJiYqL9cfDgwSKuEgAAAIXFpTgPHhgYKGdnZyUkJDgsT0hIUHBwcI7bvvvuu3rzzTf1448/qmbNmtm2c3d3l7u7e4HUCwAAgBtLsfbMurm5KSoqSsuXL7cvS0tL0/Lly9WwYcNst3v77bc1cuRILVmyRPXq1SuKUgEAAHADKtaeWUmKiYlR165dVa9ePdWvX1/jxo3TuXPn1L17d0lSly5dVK5cOY0ePVqS9NZbb2nIkCH68ssvFRYWZh9b6+3tLW9v72J7HQAAACh6xR5mO3TooGPHjmnIkCE6evSoateurSVLlthvCjtw4ICcnP7XgfzRRx8pOTlZjzzyiMN+hg4dqmHDhhVl6QAAAChmxT7PbFFjnlkAAIAbm2XmmQUAAACuB2EWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZLsVdAAAAKFqpqam6fPlycZeBm5ybm5ucnK6/X5UwCwDATcIYo6NHj+r06dPFXQogJycnVaxYUW5ubte1H8IsAAA3iYwgW7p0aXl5eclmsxV3SbhJpaWl6fDhwzpy5IgqVKhwXdciYRYAgJtAamqqPcjecsstxV0OoFKlSunw4cNKSUmRq6trvvfDDWAAANwEMsbIenl5FXMlQLqM4QWpqanXtR/CLAAANxGGFuBGUVDXImEWAAAAlkWYBQAAuA42m03ffvttoR7jzjvv1PPPP1+ox7AqwiwAALCEdevWydnZWa1bt87ztmFhYRo3blzBF3UNbdu2VcuWLbNct2rVKtlsNm3btq2Iq/pnIcwCAIA8SUuTdu2Sfvst/WdaWtEcd8qUKerbt69Wrlypw4cPF81Br1PPnj21bNkyHTp0KNO6adOmqV69eqpZs2YxVPbPQZgFAAC5tnmzFBMj9e0rDRiQ/jMmJn15YTp79qxmz56tp59+Wq1bt9b06dMztfnPf/6j22+/XR4eHgoMDNSDDz4oKf0j+v379+uFF16QzWaz33g0bNgw1a5d22Ef48aNU1hYmP35b7/9pnvuuUeBgYHy8/NTs2bNtGnTplzX3aZNG5UqVSpTvWfPntXcuXPVs2dPnThxQh07dlS5cuXk5eWlyMhIffXVVznuN6uhDf7+/g7HOXjwoNq3by9/f38FBATogQce0L59++zrV6xYofr166tEiRLy9/fXHXfcof379+f6td0oCLMAACBXNm+WRoyQNm6UAgKkypXTf27cmL68MAPtnDlzVK1aNVWtWlWdOnXS1KlTZYyxr1+0aJEefPBBtWrVSps3b9by5ctVv359SdL8+fNVvnx5jRgxQkeOHNGRI0dyfdwzZ86oa9euWr16tX799VdVrlxZrVq10pkzZ3K1vYuLi7p06aLp06c71Dt37lylpqaqY8eOunjxoqKiorRo0SL98ccfevLJJ9W5c2etX78+13Ve7fLly4qOjpaPj49WrVqlNWvWyNvbWy1btlRycrJSUlLUrl07NWvWTNu2bdO6dev05JNPWnK2C740AQAAXFNamjRjhnT8uFS9upSReXx905/HxkqffSbVqiU5FUJX2ZQpU9SpUydJUsuWLZWYmKhffvlFd955pyTpjTfe0GOPPabhw4fbt6lVq5YkKSAgQM7OzvLx8VFwcHCejnvXXXc5PP/kk0/k7++vX375RW3atMnVPnr06KF33nnHod5p06bp4Ycflp+fn/z8/DRgwAB7+759+2rp0qWaM2eOPZDn1ezZs5WWlqbJkyfbA+q0adPk7++vFStWqF69ekpMTFSbNm0UHh4uSapevXq+jlXc6JkFAADXFBeXHljLl/9fkM1gs6Uv3749vV1B27lzp9avX6+OHTtKSu/t7NChg6ZMmWJvs2XLFt19990FfuyEhAT17t1blStXlp+fn3x9fXX27FkdOHAg1/uoVq2aGjVqpKlTp0qS4uLitGrVKvXs2VNS+pcGjBw5UpGRkQoICJC3t7eWLl2ap2NcbevWrYqLi5OPj4+8vb3l7e2tgIAAXbx4UXv27FFAQIC6deum6OhotW3bVu+//36eeqxvJPTMAgCAa0pMlC5elEqUyHq9l5d0+HB6u4I2ZcoUpaSkqGzZsvZlxhi5u7trwoQJ8vPzk6enZ5736+Tk5PDRv/S/b0rL0LVrV504cULvv/++QkND5e7uroYNGyo5OTlPx+rZs6f69u2riRMnatq0aQoPD1ezZs0kSe+8847ef/99jRs3TpGRkSpRooSef/75HI9hs9lyrP3s2bOKiorSzJkzM21bqlQpSek9tf369dOSJUs0e/Zsvfbaa1q2bJn+9a9/5em1FTd6ZgEAwDX5+UkeHtK5c1mvP38+fb2fX8EeNyUlRZ999pnGjBmjLVu22B9bt25V2bJl7TdK1axZU8uXL892P25ubpm+NrVUqVI6evSoQyjcsmWLQ5s1a9aoX79+atWqlW699Va5u7vr+PHjeX4d7du3l5OTk7788kt99tln6tGjh/3j/zVr1uiBBx5Qp06dVKtWLVWqVEm7du3KcX+lSpVy6EndvXu3zp8/b39et25d7d69W6VLl1ZERITDw++Kk1SnTh0NGjRIa9eu1W233aYvv/wyz6+tuBFmAQDANUVEpI+NPXRIuqpDUMakL69RI71dQVq4cKFOnTqlnj176rbbbnN4PPzww/ahBkOHDtVXX32loUOHKjY2Vr///rveeust+37CwsK0cuVK/fXXX/Yweuedd+rYsWN6++23tWfPHk2cOFHff/+9w/ErV66szz//XLGxsfrvf/+rJ554Il+9wN7e3urQoYMGDRqkI0eOqFu3bg7HWLZsmdauXavY2Fj16dNHCQkJOe7vrrvu0oQJE7R582Zt2LBBTz31lFxdXe3rn3jiCQUGBuqBBx7QqlWrFB8frxUrVqhfv346dOiQ4uPjNWjQIK1bt0779+/XDz/8oN27d1ty3CxhFgAAXJOTk9S1qxQYmD52NilJSklJ/xkbm768S5eCv/lrypQpatGihUNvYoaHH35YGzZs0LZt23TnnXdq7ty5WrBggWrXrq277rrLYTaAESNGaN++fQoPD7d/zF69enV9+OGHmjhxomrVqqX169c73IiVcfxTp06pbt266ty5s/r166fSpUvn67X07NlTp06dUnR0tMOQiddee01169ZVdHS07rzzTgUHB6tdu3Y57mvMmDEKCQlRkyZN9Pjjj2vAgAHy8vKyr/fy8tLKlStVoUIFPfTQQ6pevbp69uypixcvytfXV15eXtqxY4cefvhhValSRU8++aSeffZZ9enTJ1+vrTjZzNUDLv7hkpKS5Ofnp8TERPn6+hZ3OQAAFImLFy8qPj5eFStWlIeHR773s3lz+qwGsbHpY2g9PNJ7ZLt0kerUKcCC8Y+X0zWZl7zGDWAAACDX6tRJn34rLi79Zi8/v/ShBYUxHReQG4RZAACQJ05OUpUqxV0FkI6/owAAAGBZhFkAAABYFmEWAAAAlkWYBQAAgGURZgEAAGBZhFkAAABYFmEWAAAAlkWYLURpadKuXdJvv6X/TEsr7ooAAEBOunXr5vBVsnfeeaeef/75Iq9jxYoVstlsOn36dKEex2az6dtvvy3UYxS2GyLMTpw4UWFhYfLw8FCDBg0cvks5K3PnzlW1atXk4eGhyMhILV68uIgqzb3Nm6WYGKlvX2nAgPSfMTHpywEAQO5169ZNNptNNptNbm5uioiI0IgRI5SSklLox54/f75GjhyZq7ZFFUCTk5MVGBioN998M8v1I0eOVFBQkC5fvlyoddwoij3Mzp49WzExMRo6dKg2bdqkWrVqKTo6Wn///XeW7deuXauOHTuqZ8+e2rx5s9q1a6d27drpjz/+KOLKs7d5szRihLRxoxQQIFWunP5z48b05QRaAIClFcNHjy1bttSRI0e0e/duvfjiixo2bJjeeeedLNsmJycX2HEDAgLk4+NTYPsrCG5uburUqZOmTZuWaZ0xRtOnT1eXLl3k6upaDNUVvWIPs2PHjlXv3r3VvXt31ahRQ5MmTZKXl5emTp2aZfv3339fLVu21EsvvaTq1atr5MiRqlu3riZMmFDElWctLU2aMUM6flyqXl3y9ZWcndN/Vq+evvyzzxhyAACwqGL66NHd3V3BwcEKDQ3V008/rRYtWmjBggWS/jc04I033lDZsmVVtWpVSdLBgwfVvn17+fv7KyAgQA888ID27dtn32dqaqpiYmLk7++vW265RS+//LKMMQ7HvXqYwaVLl/TKK68oJCRE7u7uioiI0JQpU7Rv3z41b95cklSyZEnZbDZ169ZNkpSWlqbRo0erYsWK8vT0VK1atTRv3jyH4yxevFhVqlSRp6enmjdv7lBnVnr27Kldu3Zp9erVDst/+eUX7d27Vz179tRvv/2me+65R4GBgfLz81OzZs20adOmbPeZVc/yli1bZLPZHOpZvXq1mjRpIk9PT4WEhKhfv346d+6cff2HH36oypUry8PDQ0FBQXrkkUdyfC3Xq1jDbHJysjZu3KgWLVrYlzk5OalFixZat25dltusW7fOob0kRUdHZ9v+0qVLSkpKcngUprg4KTZWKl9estkc19ls6cu3b09vBwCApdxAHz16eno69MAuX75cO3fu1LJly7Rw4UJdvnxZ0dHR8vHx0apVq7RmzRp5e3urZcuW9u3GjBmj6dOna+rUqVq9erVOnjypb775JsfjdunSRV999ZXGjx+v2NhYffzxx/L29lZISIi+/vprSdLOnTt15MgRvf/++5Kk0aNH67PPPtOkSZP0559/6oUXXlCnTp30yy+/SEoP3Q899JDatm2rLVu2qFevXho4cGCOdURGRur222/P1Pk3bdo0NWrUSNWqVdOZM2fUtWtXrV69Wr/++qsqV66sVq1a6cyZM3l7s6+wZ88etWzZUg8//LC2bdum2bNna/Xq1XruueckSRs2bFC/fv00YsQI7dy5U0uWLFHTpk3zfbzccCnUvV/D8ePHlZqaqqCgIIflQUFB2rFjR5bbHD16NMv2R48ezbL96NGjNXz48IIpOBcSE6WLF6USJbJe7+UlHT6c3g4AAMu4+qPHjB6bjI8eY2PTP3qsVUtyKry+MmOMli9frqVLl6pv37725SVKlNDkyZPl5uYmSfriiy+UlpamyZMny/b/tU6bNk3+/v5asWKF7r33Xo0bN06DBg3SQw89JEmaNGmSli5dmu2xd+3apTlz5mjZsmX2jrVKlSrZ1wcEBEiSSpcuLX9/f0npnWqjRo3Sjz/+qIYNG9q3Wb16tT7++GM1a9ZMH330kcLDwzVmzBhJUtWqVfX777/rrbfeyvG96NmzpwYMGKDx48fL29tbZ86c0bx58zR+/HhJ0l133eXQ/pNPPpG/v79++eUXtWnTJsd9Z2f06NF64okn7L3VlStX1vjx4+2v48CBAypRooTatGkjHx8fhYaGqk6dOvk6Vm4V+zCDwjZo0CAlJibaHwcPHizU4/n5SR4e0hW97Q7On09f7+dXqGUAAFCwivmjx4ULF8rb21seHh6677771KFDBw0bNsy+PjIy0h5kJWnr1q2Ki4uTj4+PvL295e3trYCAAF28eFF79uxRYmKijhw5ogYNGti3cXFxUb169bKtYcuWLXJ2dlazZs1yXXdcXJzOnz+ve+65x16Ht7e3PvvsM+3Zs0eSFBsb61CHJHvwzUnHjh2VmpqqOXPmSEq/D8nJyUkdOnSQJCUkJKh3796qXLmy/Pz85Ovrq7Nnz+rAgQO5rv9qW7du1fTp0x1eS3R0tNLS0hQfH6977rlHoaGhqlSpkjp37qyZM2fq/Pnz+T5ebhRrz2xgYKCcnZ2VkJDgsDwhIUHBwcFZbhMcHJyn9u7u7nJ3dy+YgnMhIiL9D9SNGx3/cJUkY6RDh6R69dLbAQBgGcX80WPz5s310Ucfyc3NTWXLlpWLi2OEKXFVXWfPnlVUVJRmzpyZaV+lSpXKVw2enp553ubs2bOSpEWLFqlcuXIO6643n/j6+uqRRx7RtGnT1KNHD02bNk3t27eXt7e3JKlr1646ceKE3n//fYWGhsrd3V0NGzbM9gY5p//vUb9y3PDVMyKcPXtWffr0Ub9+/TJtX6FCBbm5uWnTpk1asWKFfvjhBw0ZMkTDhg3Tb7/9Zu+tLmjF2jPr5uamqKgoLV++3L4sLS1Ny5cvz/YvkoYNGzq0l6Rly5bl6i+YouDkJHXtKgUGpv8Bm5QkpaSk/4yNTV/epUuhfgIDAEDBK+aPHkuUKKGIiAhVqFAhU5DNSt26dbV7926VLl1aERERDg8/Pz/5+fmpTJky+u9//2vfJiUlRRs3bsx2n5GRkUpLS7OPdb1aRs9wamqqfVmNGjXk7u6uAwcOZKojJCREklS9evVM05L++uuv13yNUvpQg9WrV2vhwoVau3atevbsaV+3Zs0a9evXT61atdKtt94qd3d3HT9+PNt9ZYT8I0eO2Jdt2bLFoU3dunW1ffv2TK8lIiLC/vpdXFzUokULvf3229q2bZv27dunn376KVevJz+KPVLFxMTo008/1YwZMxQbG6unn35a586dU/fu3SWlD7QeNGiQvX3//v21ZMkSjRkzRjt27NCwYcO0YcMG+8DjG0GdOtKQIVJUlHTyZPonLidPpvfIDhmSvh4AAEvJ+Ojx0KH0jxqvlPHRY40aN8xHj0888YQCAwP1wAMPaNWqVYqPj9eKFSvUr18/HTp0SFJ6pnjzzTf17bffaseOHXrmmWdynCM2LCxMXbt2VY8ePfTtt9/a95nxMX9oaKhsNpsWLlyoY8eO6ezZs/Lx8dGAAQP0wgsvaMaMGdqzZ482bdqkDz74QDNmzJAkPfXUU9q9e7deeukl7dy5U19++aWmT5+eq9fZtGlTRUREqEuXLqpWrZoaNWpkX1e5cmV9/vnnio2N1X//+1898cQTOfYuZwTsYcOGaffu3Vq0aJF9HG+GV155RWvXrtVzzz2nLVu2aPfu3fruu+/sOWzhwoUaP368tmzZov379+uzzz5TWlqafYaJQmFuAB988IGpUKGCcXNzM/Xr1ze//vqrfV2zZs1M165dHdrPmTPHVKlSxbi5uZlbb73VLFq0KNfHSkxMNJJMYmJiQZWfrdRUY3buNGb9+vSfqamFfkgAALJ04cIFs337dnPhwoX872TTJmPatTOmcWNjHnvMmB490n82bpy+fNOmgiv4Cl27djUPPPBAntcfOXLEdOnSxQQGBhp3d3dTqVIl07t3b3sGuHz5sunfv7/x9fU1/v7+JiYmxnTp0sVhX82aNTP9+/e3P79w4YJ54YUXTJkyZYybm5uJiIgwU6dOta8fMWKECQ4ONjabzZ5f0tLSzLhx40zVqlWNq6urKVWqlImOjja//PKLfbv//Oc/JiIiwri7u5smTZqYqVOnGknm1KlT13x/Ro0aZSSZt99+22H5pk2bTL169YyHh4epXLmymTt3rgkNDTXvvfeevY0k880339ifr1692kRGRhoPDw/TpEkTM3fuXCPJxMfH29usX7/e3HPPPcbb29uUKFHC1KxZ07zxxhvGGGNWrVplmjVrZkqWLGk8PT1NzZo1zezZs7OsO6drMi95zfb/L+SmkZSUJD8/PyUmJsrX17e4ywEAoEhcvHhR8fHxqlixojw8PPK/o82b02c1iI1NH0Pr4ZHeI9ulCx89Ik9yuibzkteK9QYwAABgMXXqpE+/FReXfrOXn1/60AJuBkExIcwCAIC8cXKSqlQp7ioASTfADWAAAABAfhFmAQAAYFmEWQAAbiI32X3fuIEV1LVImAUA4Cbg6uoqSYX+1aJAbmV8E5mzs/N17YcbwAAAuAk4OzvL399ff//9tyTJy8tLtiu/cx0oQmlpaTp27Ji8vLxy9Y1uOSHMAgBwkwgODpYke6AFipOTk5MqVKhw3X9UEWYBALhJ2Gw2lSlTRqVLl9bly5eLuxzc5Nzc3ORUAPMTE2YBALjJODs7X/c4ReBGwQ1gAAAAsCzCLAAAACyLMAsAAADLuunGzGZM0JuUlFTMlQAAACArGTktN1+scNOF2TNnzkiSQkJCirkSAAAA5OTMmTPy8/PLsY3N3GTfa5eWlqbDhw/Lx8enSCaLTkpKUkhIiA4ePChfX99CPx4KHufQ+jiH1sc5tDbOn/UV9Tk0xujMmTMqW7bsNafvuul6Zp2cnFS+fPkiP66vry+/wBbHObQ+zqH1cQ6tjfNnfUV5Dq/VI5uBG8AAAABgWYRZAAAAWBZhtpC5u7tr6NChcnd3L+5SkE+cQ+vjHFof59DaOH/WdyOfw5vuBjAAAAD8c9AzCwAAAMsizAIAAMCyCLMAAACwLMIsAAAALIswWwAmTpyosLAweXh4qEGDBlq/fn2O7efOnatq1arJw8NDkZGRWrx4cRFViuzk5Rx++umnatKkiUqWLKmSJUuqRYsW1zznKHx5/T3MMGvWLNlsNrVr165wC8Q15fUcnj59Ws8++6zKlCkjd3d3ValShX9Pi1Fez9+4ceNUtWpVeXp6KiQkRC+88IIuXrxYRNXiaitXrlTbtm1VtmxZ2Ww2ffvtt9fcZsWKFapbt67c3d0VERGh6dOnF3qdWTK4LrNmzTJubm5m6tSp5s8//zS9e/c2/v7+JiEhIcv2a9asMc7Ozubtt98227dvN6+99ppxdXU1v//+exFXjgx5PYePP/64mThxotm8ebOJjY013bp1M35+fubQoUNFXDky5PUcZoiPjzflypUzTZo0MQ888EDRFIss5fUcXrp0ydSrV8+0atXKrF692sTHx5sVK1aYLVu2FHHlMCbv52/mzJnG3d3dzJw508THx5ulS5eaMmXKmBdeeKGIK0eGxYsXm1dffdXMnz/fSDLffPNNju337t1rvLy8TExMjNm+fbv54IMPjLOzs1myZEnRFHwFwux1ql+/vnn22Wftz1NTU03ZsmXN6NGjs2zfvn1707p1a4dlDRo0MH369CnUOpG9vJ7Dq6WkpBgfHx8zY8aMwioR15Cfc5iSkmIaNWpkJk+ebLp27UqYLWZ5PYcfffSRqVSpkklOTi6qEpGDvJ6/Z5991tx1110Oy2JiYswdd9xRqHUid3ITZl9++WVz6623Oizr0KGDiY6OLsTKssYwg+uQnJysjRs3qkWLFvZlTk5OatGihdatW5flNuvWrXNoL0nR0dHZtkfhys85vNr58+d1+fJlBQQEFFaZyEF+z+GIESNUunRp9ezZsyjKRA7ycw4XLFighg0b6tlnn1VQUJBuu+02jRo1SqmpqUVVNv5ffs5fo0aNtHHjRvtQhL1792rx4sVq1apVkdSM63cj5RmXIj/iP8jx48eVmpqqoKAgh+VBQUHasWNHltscPXo0y/ZHjx4ttDqRvfycw6u98sorKlu2bKZfahSN/JzD1atXa8qUKdqyZUsRVIhryc853Lt3r3766Sc98cQTWrx4seLi4vTMM8/o8uXLGjp0aFGUjf+Xn/P3+OOP6/jx42rcuLGMMUpJSdFTTz2lf//730VRMgpAdnkmKSlJFy5ckKenZ5HVQs8scB3efPNNzZo1S9988408PDyKuxzkwpkzZ9S5c2d9+umnCgwMLO5ykE9paWkqXbq0PvnkE0VFRalDhw569dVXNWnSpOIuDbmwYsUKjRo1Sh9++KE2bdqk+fPna9GiRRo5cmRxlwYLomf2OgQGBsrZ2VkJCQkOyxMSEhQcHJzlNsHBwXlqj8KVn3OY4d1339Wbb76pH3/8UTVr1izMMpGDvJ7DPXv2aN++fWrbtq19WVpamiTJxcVFO3fuVHh4eOEWDQf5+T0sU6aMXF1d5ezsbF9WvXp1HT16VMnJyXJzcyvUmvE/+Tl/gwcPVufOndWrVy9JUmRkpM6dO6cnn3xSr776qpyc6Gu70WWXZ3x9fYu0V1aiZ/a6uLm5KSoqSsuXL7cvS0tL0/Lly9WwYcMst2nYsKFDe0latmxZtu1RuPJzDiXp7bff1siRI7VkyRLVq1evKEpFNvJ6DqtVq6bff/9dW7ZssT/uv/9+NW/eXFu2bFFISEhRlg/l7/fwjjvuUFxcnP0PEUnatWuXypQpQ5AtYvk5f+fPn88UWDP+MDHGFF6xKDA3VJ4p8lvO/mFmzZpl3N3dzfTp08327dvNk08+afz9/c3Ro0eNMcZ07tzZDBw40N5+zZo1xsXFxbz77rsmNjbWDB06lKm5illez+Gbb75p3NzczLx588yRI0fsjzNnzhTXS7jp5fUcXo3ZDIpfXs/hgQMHjI+Pj3nuuefMzp07zcKFC03p0qXN66+/Xlwv4aaW1/M3dOhQ4+PjY7766iuzd+9e88MPP5jw8HDTvn374noJN70zZ86YzZs3m82bNxtJZuzYsWbz5s1m//79xhhjBg4caDp37mxvnzE110svvWRiY2PNxIkTmZrLyj744ANToUIF4+bmZurXr29+/fVX+7pmzZqZrl27OrSfM2eOqVKlinFzczO33nqrWbRoURFXjKvl5RyGhoYaSZkeQ4cOLfrCYZfX38MrEWZvDHk9h2vXrjUNGjQw7u7uplKlSuaNN94wKSkpRVw1MuTl/F2+fNkMGzbMhIeHGw8PDxMSEmKeeeYZc+rUqaIvHMYYY37++ecs/9+Wcd66du1qmjVrlmmb2rVrGzc3N1OpUiUzbdq0Iq/bGGNsxtCfDwAAAGtizCwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAWIjNZtO3335b3GVo2LBhql27dnGXAQCEWQC40rFjx/T000+rQoUKcnd3V3BwsKKjo7VmzZriLq1A7Nu3TzabTVu2bCnuUgCgQLgUdwEAcCN5+OGHlZycrBkzZqhSpUpKSEjQ8uXLdeLEieIuDQCQBXpmAeD/nT59WqtWrdJbb72l5s2bKzQ0VPXr19egQYN0//3329uNHTtWkZGRKlGihEJCQvTMM8/o7Nmz9vXTp0+Xv7+/Fi5cqKpVq8rLy0uPPPKIzp8/rxkzZigsLEwlS5ZUv379lJqaat8uLCxMI0eOVMeOHVWiRAmVK1dOEydOzLHmgwcPqn379vL391dAQIAeeOAB7du3L9evecWKFbLZbFq+fLnq1asnLy8vNWrUSDt37nRo9+abbyooKEg+Pj7q2bOnLl68mGlfkydPVvXq1eXh4aFq1arpww8/tK/r0aOHatasqUuXLkmSkpOTVadOHXXp0iXXtQJAVgizAPD/vL295e3trW+//dYeurLi5OSk8ePH688//9SMGTP0008/6eWXX3Zoc/78eY0fP16zZs3SkiVLtGLFCj344INavHixFi9erM8//1wff/yx5s2b57DdO++8o1q1amnz5s0aOHCg+vfvr2XLlmVZx+XLlxUdHS0fHx+tWrVKa9askbe3t1q2bKnk5OQ8vfZXX31VY8aM0YYNG+Ti4qIePXrY182ZM0fDhg3TqFGjtGHDBpUpU8YhqErSzJkzNWTIEL3xxhuKjY3VqFGjNHjwYM2YMUOSNH78eJ07d04DBw60H+/06dOaMGFCnuoEgEwMAMBu3rx5pmTJksbDw8M0atTIDBo0yGzdujXHbebOnWtuueUW+/Np06YZSSYuLs6+rE+fPsbLy8ucOXPGviw6Otr06dPH/jw0NNS0bNnSYd8dOnQw9913n/25JPPNN98YY4z5/PPPTdWqVU1aWpp9/aVLl4ynp6dZunRplrXGx8cbSWbz5s3GGGN+/vlnI8n8+OOP9jaLFi0yksyFCxeMMcY0bNjQPPPMMw77adCggalVq5b9eXh4uPnyyy8d2owcOdI0bNjQ/nzt2rXG1dXVDB482Li4uJhVq1ZlWSMA5AU9swBwhYcffliHDx/WggUL1LJlS61YsUJ169bV9OnT7W1+/PFH3X333SpXrpx8fHzUuXNnnThxQufPn7e38fLyUnh4uP15UFCQwsLC5O3t7bDs77//djh+w4YNMz2PjY3NstatW7cqLi5OPj4+9l7lgIAAXbx4UXv27MnT665Zs6b9v8uUKSNJ9tpiY2PVoEGDbOs8d+6c9uzZo549e9rr8Pb21uuvv+5QR8OGDTVgwACNHDlSL774oho3bpynGgEgK9wABgBX8fDw0D333KN77rlHgwcPVq9evTR06FB169ZN+/btU5s2bfT000/rjTfeUEBAgFavXq2ePXsqOTlZXl5ekiRXV1eHfdpstiyXpaWl5bvOs2fPKioqSjNnzsy0rlSpUnna15W12Ww2Scp1bRnjhT/99NNModfZ2dn+32lpaVqzZo2cnZ0VFxeXp/oAIDv0zALANdSoUUPnzp2TJG3cuFFpaWkaM2aM/vWvf6lKlSo6fPhwgR3r119/zfS8evXqWbatW7eudu/erdKlSysiIsLh4efnV2A1Va9eXf/973+zrTMoKEhly5bV3r17M9VRsWJFe7t33nlHO3bs0C+//KIlS5Zo2rRpBVYjgJsXYRYA/t+JEyd011136YsvvtC2bdsUHx+vuXPn6u2339YDDzwgSYqIiNDly5f1wQcfaO/evfr88881adKkAqthzZo1evvtt7Vr1y5NnDhRc+fOVf/+/bNs+8QTTygwMFAPPPCAVq1apfj4eK1YsUL9+vXToUOHCqym/v37a+rUqZo2bZp27dqloUOH6s8//3RoM3z4cI0ePVrjx4/Xrl279Pvvv2vatGkaO3asJGnz5s0aMmSIJk+erDvuuENjx45V//79tXfv3gKrE8DNiTALAP/P29tbDRo00HvvvaemTZvqtttu0+DBg9W7d2/7Xfe1atXS2LFj9dZbb+m2227TzJkzNXr06AKr4cUXX9SGDRtUp04dvf766xo7dqyio6OzbOvl5aWVK1eqQoUKeuihh1S9enX7tFm+vr4FVlOHDh00ePBgvfzyy4qKitL+/fv19NNPO7Tp1auXJk+erGnTpikyMlLNmjXT9OnTVbFiRV28eFGdOnVSt27d1LZtW0nSk08+qebNm6tz584O05MBQF7ZjDGmuIsAAKTPM/v888/r+eefL+5SAMAy6JkFAACAZRFmAQAAYFkMMwAAAIBl0TMLAAAAyyLMAgAAwLIIswAAALAswiwAAAAsizALAAAAyyLMAgAAwLIIswAAALAswiwAAAAs6/8AIDmyvRqoREcAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Vectorization\n"
      ],
      "metadata": {
        "id": "Sg65AbXDgwvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "rhDhRiJS8h5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating a 1D Array\n"
      ],
      "metadata": {
        "id": "7xXH9Id48ogZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = np.array([1, 2, 3, 4, 5])\n",
        "print(vector)\n",
        "print(\"Shape:\", vector.shape)  # Output: (5,)\n",
        "print(\"Dimension:\", vector.ndim)  # Output: 1\n"
      ],
      "metadata": {
        "id": "m1SVAaWeg1PN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c340d7ba-0cb0-401d-878c-bd6c6448ba0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4 5]\n",
            "Shape: (5,)\n",
            "Dimension: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Representing 2D Matrices (2×3 and 4×5 examples)"
      ],
      "metadata": {
        "id": "7LzO6LrF81D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_2x3 = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6]\n",
        "])\n",
        "\n",
        "\n",
        "print(matrix_2x3)\n",
        "print(\"Shape:\", matrix_2x3.shape)  # Output: (2, 3)\n",
        "print(\"Dimension:\", matrix_2x3.ndim)  # Output: 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYIwABUJ9Adh",
        "outputId": "1989bdfc-9849-4370-e60d-cb54d692f24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "Shape: (2, 3)\n",
            "Dimension: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_4x5 = np.array([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [6, 7, 8, 9, 10],\n",
        "    [11, 12, 13, 14, 15],\n",
        "    [16, 17, 18, 19, 20]\n",
        "])\n",
        "\n",
        "\n",
        "print(matrix_4x5)\n",
        "print(\"Shape:\", matrix_4x5.shape)  # Output: (4, 5)\n",
        "print(\"Dimension:\", matrix_4x5.ndim)  # Output: 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUNOSgBI9X7p",
        "outputId": "948516ca-48e3-450a-bd44-f47a80059618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  2  3  4  5]\n",
            " [ 6  7  8  9 10]\n",
            " [11 12 13 14 15]\n",
            " [16 17 18 19 20]]\n",
            "Shape: (4, 5)\n",
            "Dimension: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Higher Dimensional Arrays (3D Example)\n"
      ],
      "metadata": {
        "id": "lvvRKn3T9rip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_3d = np.array([\n",
        "    [[1, 2, 3], [4, 5, 6]],  # First 2x3 matrix\n",
        "    [[7, 8, 9], [10, 11, 12]]  # Second 2x3 matrix\n",
        "])\n",
        "\n",
        "\n",
        "print(tensor_3d)\n",
        "print(\"Shape:\", tensor_3d.shape)  # Output: (2, 2, 3)\n",
        "print(\"Dimension:\", tensor_3d.ndim)  # Output: 3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXZFc5bg-AoM",
        "outputId": "174dbd5d-ea27-4afd-d096-e0e3d9ad2e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 1  2  3]\n",
            "  [ 4  5  6]]\n",
            "\n",
            " [[ 7  8  9]\n",
            "  [10 11 12]]]\n",
            "Shape: (2, 2, 3)\n",
            "Dimension: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Broadcasting in NumPy\n"
      ],
      "metadata": {
        "id": "t4jdaxjZBBga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "scalar = 10\n",
        "\n",
        "\n",
        "result = matrix + scalar  # Broadcasting scalar to matrix\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng7dsIJCBWLs",
        "outputId": "069c55b4-06f8-4705-d421-cc7056eca270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[11 12 13]\n",
            " [14 15 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Broadcasting a 1D Array to a 2D Matrix\n"
      ],
      "metadata": {
        "id": "ztDzwZDjCl_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6]\n",
        "])\n",
        "\n",
        "\n",
        "vector = np.array([10, 20, 30])  # 1D array\n",
        "\n",
        "\n",
        "result = matrix + vector  # Broadcasting 1D to 2D\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jrvvM1MCfKr",
        "outputId": "124e95ee-67c1-4ca0-f433-22fc45a48028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[11 22 33]\n",
            " [14 25 36]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example of Broadcasting Error\n"
      ],
      "metadata": {
        "id": "vK1buBD-C7Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "wrong_vector = np.array([10, 20])  # Only 2 elements, should be 3!\n",
        "\n",
        "\n",
        "result = matrix + wrong_vector  # Error!\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "G0GWc-vqDErt",
        "outputId": "aedb9ac7-4820-4f08-a0eb-c78058e52a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (2,3) (2,) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-97397858a15c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwrong_vector\u001b[0m  \u001b[0;31m# Error!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,3) (2,) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Building a Perceptron"
      ],
      "metadata": {
        "id": "y3ELarRiMaqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Define Activation Function (Step Function)\n",
        "def step_function(x):\n",
        "    return 1 if x >= 0 else 0  # Outputs 1 if weighted sum >= 0, else 0\n",
        "\n",
        "# Step 2: Train Perceptron Model\n",
        "def train_perceptron(X, Y, learning_rate=0.1, epochs=10):\n",
        "    num_features = X.shape[1]  # Number of input features (2 for AND gate)\n",
        "    weights = np.zeros(num_features)  # Initialize weights to zero\n",
        "    bias = 0  # Initialize bias to zero\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(len(X)):\n",
        "            weighted_sum = np.dot(weights, X[i]) + bias  # Compute weighted sum\n",
        "            prediction = step_function(weighted_sum)  # Apply activation function\n",
        "\n",
        "            # Compute error\n",
        "            error = Y[i] - prediction  # Difference between actual and predicted\n",
        "\n",
        "            # Update weights and bias using perceptron learning rule\n",
        "            weights += learning_rate * error * X[i]\n",
        "            bias += learning_rate * error\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Step 3: Define Testing Function\n",
        "def predict(X, weights, bias):\n",
        "    predictions = []\n",
        "    for i in range(len(X)):\n",
        "        weighted_sum = np.dot(weights, X[i]) + bias  # Compute weighted sum\n",
        "        output = step_function(weighted_sum)  # Apply activation function\n",
        "        predictions.append(output)\n",
        "    return predictions\n",
        "\n",
        "# Step 4: Define Training Data (AND Gate Inputs and Outputs)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
        "Y = np.array([0, 0, 0, 1])  # AND Function Outputs\n",
        "\n",
        "# Step 5: Train the Perceptron\n",
        "weights, bias = train_perceptron(X, Y, learning_rate=0.1, epochs=10)\n",
        "\n",
        "# Step 6: Test the Trained Perceptron\n",
        "predictions = predict(X, weights, bias)\n",
        "\n",
        "# Step 7: Print Results\n",
        "print(f\"Trained Weights: {weights}\")\n",
        "print(f\"Trained Bias: {bias}\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]}, Output: {predictions[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxGmpPcfMrk9",
        "outputId": "02ecf06a-5068-4d9f-e002-2d798fd189b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Weights: [0.2 0.1]\n",
            "Trained Bias: -0.20000000000000004\n",
            "Input: [0 0], Output: 0\n",
            "Input: [0 1], Output: 0\n",
            "Input: [1 0], Output: 0\n",
            "Input: [1 1], Output: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Forward Propagation"
      ],
      "metadata": {
        "id": "6XjtPJbeeQ6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Dense layer function\n",
        "def dense(a_in, W, b):\n",
        "\n",
        "    z = np.dot(W, a_in) + b  # Compute weighted sum + bias\n",
        "    a_out = sigmoid(z)  # Apply activation function (Sigmoid)\n",
        "    return a_out\n",
        "\n",
        "# Define weight matrices and bias vectors\n",
        "W1 = np.array([[1, 2],    # 3 neurons in hidden layer 1 (3x2 matrix)\n",
        "               [-3, 4],\n",
        "               [5, 6]])\n",
        "\n",
        "b1 = np.array([-1, 1, 2])  # Bias for hidden layer 1\n",
        "\n",
        "W2 = np.array([[0.5, -1, 2],   # 2 neurons in hidden layer 2 (2x3 matrix)\n",
        "               [1, 3, -2]])\n",
        "\n",
        "b2 = np.array([0, -1])  # Bias for hidden layer 2\n",
        "\n",
        "W3 = np.array([[2, -1]])  # Output layer (1x2 matrix)\n",
        "\n",
        "b3 = np.array([0.5])  # Bias for output layer\n",
        "\n",
        "# Input feature vector (2D)\n",
        "x = np.array([-2, 4])\n",
        "\n",
        "# Forward propagation\n",
        "a1 = dense(x, W1, b1)  # First hidden layer (3 neurons)\n",
        "a2 = dense(a1, W2, b2) # Second hidden layer (2 neurons)\n",
        "output = dense(a2, W3, b3)  # Output layer (1 neuron)\n",
        "\n",
        "print(\"Final Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVnzynB8e-VD",
        "outputId": "3133cfd3-ac22-44df-bde4-20040fce1c35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Output: [0.80288258]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.Backward Propagation"
      ],
      "metadata": {
        "id": "XghTRVdAgRJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Input data (2 features)\n",
        "x = np.array([[-2], [4]])  # Shape: (2,1)\n",
        "\n",
        "# True target output\n",
        "y_true = np.array([[1]])  # Shape: (1,1)\n",
        "\n",
        "# Define neural network parameters\n",
        "W1 = np.array([[1, 2], [-3, 4], [5, 6]], dtype=float)  # Shape: (3,2)\n",
        "b1 = np.array([[-1], [1], [2]], dtype=float)          # Shape: (3,1)\n",
        "\n",
        "W2 = np.array([[0.5, -1, 2], [1, 3, -2]], dtype=float)  # Shape: (2,3)\n",
        "b2 = np.array([[0], [-1]], dtype=float)                # Shape: (2,1)\n",
        "\n",
        "W3 = np.array([[2, -1]], dtype=float)  # Shape: (1,2)\n",
        "b3 = np.array([[0.5]], dtype=float)    # Shape: (1,1)\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Derivative of Sigmoid\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Number of iterations\n",
        "epochs = 100  # Keeping it small for readability\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward Propagation\n",
        "    z1 = np.dot(W1, x) + b1  # (3,2) . (2,1) -> (3,1)\n",
        "    a1 = sigmoid(z1)  # Shape: (3,1)\n",
        "\n",
        "    z2 = np.dot(W2, a1) + b2  # (2,3) . (3,1) -> (2,1)\n",
        "    a2 = sigmoid(z2)  # Shape: (2,1)\n",
        "\n",
        "    z3 = np.dot(W3, a2) + b3  # (1,2) . (2,1) -> (1,1)\n",
        "    a3 = sigmoid(z3)  # Final Output\n",
        "\n",
        "    # Compute Loss (Mean Squared Error)\n",
        "    loss = 0.5 * np.sum((a3 - y_true) ** 2)\n",
        "\n",
        "    # Backpropagation\n",
        "    dL_da3 = a3 - y_true  # Derivative of loss w.r.t output\n",
        "    dL_dz3 = dL_da3 * sigmoid_derivative(z3)  # Chain rule\n",
        "\n",
        "    dL_dW3 = np.dot(dL_dz3, a2.T)  # (1,1) . (1,2) -> (1,2)\n",
        "    dL_db3 = dL_dz3  # Bias gradient same as activation gradient\n",
        "\n",
        "    dL_da2 = np.dot(W3.T, dL_dz3)  # (2,1) . (1,1) -> (2,1)\n",
        "    dL_dz2 = dL_da2 * sigmoid_derivative(z2)  # (2,1) * (2,1)\n",
        "\n",
        "    dL_dW2 = np.dot(dL_dz2, a1.T)  # (2,1) . (1,3) -> (2,3)\n",
        "    dL_db2 = dL_dz2  # Bias gradient\n",
        "\n",
        "    dL_da1 = np.dot(W2.T, dL_dz2)  # (3,2) . (2,1) -> (3,1)\n",
        "    dL_dz1 = dL_da1 * sigmoid_derivative(z1)  # (3,1) * (3,1)\n",
        "\n",
        "    dL_dW1 = np.dot(dL_dz1, x.T)  # (3,1) . (1,2) -> (3,2)\n",
        "    dL_db1 = dL_dz1  # Bias gradient\n",
        "\n",
        "    # Gradient Descent - Updating Weights & Biases\n",
        "    W3 -= learning_rate * dL_dW3\n",
        "    b3 -= learning_rate * dL_db3\n",
        "    W2 -= learning_rate * dL_dW2\n",
        "    b2 -= learning_rate * dL_db2\n",
        "    W1 -= learning_rate * dL_dW1\n",
        "    b1 -= learning_rate * dL_db1\n",
        "\n",
        "    # Print weights, biases, and a3 at each step\n",
        "    print(f\"\\nEpoch {epoch}, Loss: {loss:.6f}\")\n",
        "    print(\"Updated Weights and Biases:\")\n",
        "    print(\"W1:\\n\", W1)\n",
        "    print(\"b1:\\n\", b1)\n",
        "    print(\"W2:\\n\", W2)\n",
        "    print(\"b2:\\n\", b2)\n",
        "    print(\"W3:\\n\", W3)\n",
        "    print(\"b3:\\n\", b3)\n",
        "    print(\"Final Output (a3):\", a3)\n",
        "\n",
        "# Final output after training\n",
        "print(\"\\nFinal Prediction:\", a3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZHs1lJ7gZYR",
        "outputId": "8cb375ea-4bc0-42e4-82ba-53808a501c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0, Loss: 0.019428\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.5 -1.   2. ]\n",
            " [ 1.   3.  -2. ]]\n",
            "b2:\n",
            " [[ 9.33e-04]\n",
            " [-1.00e+00]]\n",
            "W3:\n",
            " [[ 2. -1.]]\n",
            "b3:\n",
            " [[0.5]]\n",
            "Final Output (a3): [[0.8]]\n",
            "\n",
            "Epoch 1, Loss: 0.019165\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.5 -1.   2. ]\n",
            " [ 1.   3.  -2. ]]\n",
            "b2:\n",
            " [[ 0.]\n",
            " [-1.]]\n",
            "W3:\n",
            " [[ 2.01 -1.  ]]\n",
            "b3:\n",
            " [[0.51]]\n",
            "Final Output (a3): [[0.8]]\n",
            "\n",
            "Epoch 2, Loss: 0.018909\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.5 -1.   2. ]\n",
            " [ 1.   3.  -2. ]]\n",
            "b2:\n",
            " [[ 0.]\n",
            " [-1.]]\n",
            "W3:\n",
            " [[ 2.01 -0.99]]\n",
            "b3:\n",
            " [[0.51]]\n",
            "Final Output (a3): [[0.81]]\n",
            "\n",
            "Epoch 3, Loss: 0.018659\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.5 -1.   2. ]\n",
            " [ 1.   3.  -2. ]]\n",
            "b2:\n",
            " [[ 0.]\n",
            " [-1.]]\n",
            "W3:\n",
            " [[ 2.01 -0.99]]\n",
            "b3:\n",
            " [[0.51]]\n",
            "Final Output (a3): [[0.81]]\n",
            "\n",
            "Epoch 4, Loss: 0.018415\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.5 -1.   2. ]\n",
            " [ 1.   3.  -2. ]]\n",
            "b2:\n",
            " [[ 0.]\n",
            " [-1.]]\n",
            "W3:\n",
            " [[ 2.01 -0.99]]\n",
            "b3:\n",
            " [[0.52]]\n",
            "Final Output (a3): [[0.81]]\n",
            "\n",
            "Epoch 5, Loss: 0.018177\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 1.    3.   -2.  ]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.  ]]\n",
            "W3:\n",
            " [[ 2.01 -0.99]]\n",
            "b3:\n",
            " [[0.52]]\n",
            "Final Output (a3): [[0.81]]\n",
            "\n",
            "Epoch 6, Loss: 0.017944\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 1.    3.   -2.  ]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.  ]]\n",
            "W3:\n",
            " [[ 2.02 -0.98]]\n",
            "b3:\n",
            " [[0.52]]\n",
            "Final Output (a3): [[0.81]]\n",
            "\n",
            "Epoch 7, Loss: 0.017716\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 1.    3.   -2.  ]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.  ]]\n",
            "W3:\n",
            " [[ 2.02 -0.98]]\n",
            "b3:\n",
            " [[0.52]]\n",
            "Final Output (a3): [[0.81]]\n",
            "\n",
            "Epoch 8, Loss: 0.017494\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.02 -0.98]]\n",
            "b3:\n",
            " [[0.53]]\n",
            "Final Output (a3): [[0.81]]\n",
            "\n",
            "Epoch 9, Loss: 0.017276\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.02 -0.98]]\n",
            "b3:\n",
            " [[0.53]]\n",
            "Final Output (a3): [[0.81]]\n",
            "\n",
            "Epoch 10, Loss: 0.017063\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.03 -0.98]]\n",
            "b3:\n",
            " [[0.53]]\n",
            "Final Output (a3): [[0.82]]\n",
            "\n",
            "Epoch 11, Loss: 0.016855\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.03 -0.97]]\n",
            "b3:\n",
            " [[0.54]]\n",
            "Final Output (a3): [[0.82]]\n",
            "\n",
            "Epoch 12, Loss: 0.016652\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.03 -0.97]]\n",
            "b3:\n",
            " [[0.54]]\n",
            "Final Output (a3): [[0.82]]\n",
            "\n",
            "Epoch 13, Loss: 0.016453\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.03 -0.97]]\n",
            "b3:\n",
            " [[0.54]]\n",
            "Final Output (a3): [[0.82]]\n",
            "\n",
            "Epoch 14, Loss: 0.016258\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.04 -0.97]]\n",
            "b3:\n",
            " [[0.54]]\n",
            "Final Output (a3): [[0.82]]\n",
            "\n",
            "Epoch 15, Loss: 0.016067\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.04 -0.97]]\n",
            "b3:\n",
            " [[0.55]]\n",
            "Final Output (a3): [[0.82]]\n",
            "\n",
            "Epoch 16, Loss: 0.015880\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.51 -0.99  2.01]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.01]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.04 -0.96]]\n",
            "b3:\n",
            " [[0.55]]\n",
            "Final Output (a3): [[0.82]]\n",
            "\n",
            "Epoch 17, Loss: 0.015698\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.04 -0.96]]\n",
            "b3:\n",
            " [[0.55]]\n",
            "Final Output (a3): [[0.82]]\n",
            "\n",
            "Epoch 18, Loss: 0.015519\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.04 -0.96]]\n",
            "b3:\n",
            " [[0.55]]\n",
            "Final Output (a3): [[0.82]]\n",
            "\n",
            "Epoch 19, Loss: 0.015343\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.05 -0.96]]\n",
            "b3:\n",
            " [[0.56]]\n",
            "Final Output (a3): [[0.82]]\n",
            "\n",
            "Epoch 20, Loss: 0.015171\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.05 -0.96]]\n",
            "b3:\n",
            " [[0.56]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 21, Loss: 0.015003\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.05 -0.96]]\n",
            "b3:\n",
            " [[0.56]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 22, Loss: 0.014838\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.05 -0.95]]\n",
            "b3:\n",
            " [[0.56]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 23, Loss: 0.014677\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.05 -0.95]]\n",
            "b3:\n",
            " [[0.57]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 24, Loss: 0.014518\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.06 -0.95]]\n",
            "b3:\n",
            " [[0.57]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 25, Loss: 0.014363\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.06 -0.95]]\n",
            "b3:\n",
            " [[0.57]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 26, Loss: 0.014210\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.06 -0.95]]\n",
            "b3:\n",
            " [[0.57]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 27, Loss: 0.014061\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.99  2.99 -2.01]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.01]]\n",
            "W3:\n",
            " [[ 2.06 -0.95]]\n",
            "b3:\n",
            " [[0.58]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 28, Loss: 0.013914\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.06 -0.94]]\n",
            "b3:\n",
            " [[0.58]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 29, Loss: 0.013770\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.07 -0.94]]\n",
            "b3:\n",
            " [[0.58]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 30, Loss: 0.013629\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.07 -0.94]]\n",
            "b3:\n",
            " [[0.58]]\n",
            "Final Output (a3): [[0.83]]\n",
            "\n",
            "Epoch 31, Loss: 0.013491\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.52 -0.98  2.02]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.02]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.07 -0.94]]\n",
            "b3:\n",
            " [[0.58]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 32, Loss: 0.013355\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.07 -0.94]]\n",
            "b3:\n",
            " [[0.59]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 33, Loss: 0.013221\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.07 -0.94]]\n",
            "b3:\n",
            " [[0.59]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 34, Loss: 0.013090\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.08 -0.93]]\n",
            "b3:\n",
            " [[0.59]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 35, Loss: 0.012962\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.08 -0.93]]\n",
            "b3:\n",
            " [[0.59]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 36, Loss: 0.012835\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.08 -0.93]]\n",
            "b3:\n",
            " [[0.6]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 37, Loss: 0.012711\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.08 -0.93]]\n",
            "b3:\n",
            " [[0.6]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 38, Loss: 0.012589\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.08 -0.93]]\n",
            "b3:\n",
            " [[0.6]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 39, Loss: 0.012470\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.08 -0.93]]\n",
            "b3:\n",
            " [[0.6]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 40, Loss: 0.012352\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.09 -0.92]]\n",
            "b3:\n",
            " [[0.6]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 41, Loss: 0.012236\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.09 -0.92]]\n",
            "b3:\n",
            " [[0.61]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 42, Loss: 0.012123\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.09 -0.92]]\n",
            "b3:\n",
            " [[0.61]]\n",
            "Final Output (a3): [[0.84]]\n",
            "\n",
            "Epoch 43, Loss: 0.012011\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.09 -0.92]]\n",
            "b3:\n",
            " [[0.61]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 44, Loss: 0.011901\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.09 -0.92]]\n",
            "b3:\n",
            " [[0.61]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 45, Loss: 0.011793\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.09 -0.92]]\n",
            "b3:\n",
            " [[0.61]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 46, Loss: 0.011687\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.1  -0.92]]\n",
            "b3:\n",
            " [[0.62]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 47, Loss: 0.011582\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.97  2.03]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.03]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.1  -0.91]]\n",
            "b3:\n",
            " [[0.62]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 48, Loss: 0.011480\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.53 -0.96  2.04]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.1  -0.91]]\n",
            "b3:\n",
            " [[0.62]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 49, Loss: 0.011378\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.1  -0.91]]\n",
            "b3:\n",
            " [[0.62]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 50, Loss: 0.011279\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.1  -0.91]]\n",
            "b3:\n",
            " [[0.62]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 51, Loss: 0.011181\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.1  -0.91]]\n",
            "b3:\n",
            " [[0.63]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 52, Loss: 0.011085\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.11 -0.91]]\n",
            "b3:\n",
            " [[0.63]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 53, Loss: 0.010990\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.98  2.98 -2.02]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.02]]\n",
            "W3:\n",
            " [[ 2.11 -0.91]]\n",
            "b3:\n",
            " [[0.63]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 54, Loss: 0.010897\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.11 -0.91]]\n",
            "b3:\n",
            " [[0.63]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 55, Loss: 0.010805\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.11 -0.9 ]]\n",
            "b3:\n",
            " [[0.63]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 56, Loss: 0.010715\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.11 -0.9 ]]\n",
            "b3:\n",
            " [[0.64]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 57, Loss: 0.010626\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.11 -0.9 ]]\n",
            "b3:\n",
            " [[0.64]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 58, Loss: 0.010538\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.12 -0.9 ]]\n",
            "b3:\n",
            " [[0.64]]\n",
            "Final Output (a3): [[0.85]]\n",
            "\n",
            "Epoch 59, Loss: 0.010452\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.12 -0.9 ]]\n",
            "b3:\n",
            " [[0.64]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 60, Loss: 0.010367\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.12 -0.9 ]]\n",
            "b3:\n",
            " [[0.64]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 61, Loss: 0.010283\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.12 -0.9 ]]\n",
            "b3:\n",
            " [[0.64]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 62, Loss: 0.010201\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.12 -0.9 ]]\n",
            "b3:\n",
            " [[0.65]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 63, Loss: 0.010120\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.12 -0.89]]\n",
            "b3:\n",
            " [[0.65]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 64, Loss: 0.010040\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.12 -0.89]]\n",
            "b3:\n",
            " [[0.65]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 65, Loss: 0.009961\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.13 -0.89]]\n",
            "b3:\n",
            " [[0.65]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 66, Loss: 0.009883\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.13 -0.89]]\n",
            "b3:\n",
            " [[0.65]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 67, Loss: 0.009807\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.54 -0.96  2.04]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.04]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.13 -0.89]]\n",
            "b3:\n",
            " [[0.65]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 68, Loss: 0.009731\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.13 -0.89]]\n",
            "b3:\n",
            " [[0.66]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 69, Loss: 0.009657\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.13 -0.89]]\n",
            "b3:\n",
            " [[0.66]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 70, Loss: 0.009583\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.13 -0.89]]\n",
            "b3:\n",
            " [[0.66]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 71, Loss: 0.009511\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.13 -0.88]]\n",
            "b3:\n",
            " [[0.66]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 72, Loss: 0.009440\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.14 -0.88]]\n",
            "b3:\n",
            " [[0.66]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 73, Loss: 0.009369\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.14 -0.88]]\n",
            "b3:\n",
            " [[0.66]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 74, Loss: 0.009300\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.14 -0.88]]\n",
            "b3:\n",
            " [[0.67]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 75, Loss: 0.009232\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.14 -0.88]]\n",
            "b3:\n",
            " [[0.67]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 76, Loss: 0.009164\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.14 -0.88]]\n",
            "b3:\n",
            " [[0.67]]\n",
            "Final Output (a3): [[0.86]]\n",
            "\n",
            "Epoch 77, Loss: 0.009098\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.14 -0.88]]\n",
            "b3:\n",
            " [[0.67]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 78, Loss: 0.009032\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.14 -0.88]]\n",
            "b3:\n",
            " [[0.67]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 79, Loss: 0.008967\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.14 -0.88]]\n",
            "b3:\n",
            " [[0.67]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 80, Loss: 0.008904\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.15 -0.87]]\n",
            "b3:\n",
            " [[0.68]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 81, Loss: 0.008841\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.15 -0.87]]\n",
            "b3:\n",
            " [[0.68]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 82, Loss: 0.008778\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.15 -0.87]]\n",
            "b3:\n",
            " [[0.68]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 83, Loss: 0.008717\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.15 -0.87]]\n",
            "b3:\n",
            " [[0.68]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 84, Loss: 0.008656\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.15 -0.87]]\n",
            "b3:\n",
            " [[0.68]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 85, Loss: 0.008597\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.97 -2.03]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.03]]\n",
            "W3:\n",
            " [[ 2.15 -0.87]]\n",
            "b3:\n",
            " [[0.68]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 86, Loss: 0.008538\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.97  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.15 -0.87]]\n",
            "b3:\n",
            " [[0.68]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 87, Loss: 0.008479\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.15 -0.87]]\n",
            "b3:\n",
            " [[0.69]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 88, Loss: 0.008422\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.16 -0.87]]\n",
            "b3:\n",
            " [[0.69]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 89, Loss: 0.008365\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.16 -0.86]]\n",
            "b3:\n",
            " [[0.69]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 90, Loss: 0.008309\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.55 -0.95  2.05]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.05]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.16 -0.86]]\n",
            "b3:\n",
            " [[0.69]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 91, Loss: 0.008254\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.56 -0.94  2.06]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.06]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.16 -0.86]]\n",
            "b3:\n",
            " [[0.69]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 92, Loss: 0.008199\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.56 -0.94  2.06]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.06]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.16 -0.86]]\n",
            "b3:\n",
            " [[0.69]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 93, Loss: 0.008145\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.56 -0.94  2.06]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.06]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.16 -0.86]]\n",
            "b3:\n",
            " [[0.69]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 94, Loss: 0.008092\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.56 -0.94  2.06]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.06]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.16 -0.86]]\n",
            "b3:\n",
            " [[0.7]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 95, Loss: 0.008039\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.56 -0.94  2.06]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.06]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.16 -0.86]]\n",
            "b3:\n",
            " [[0.7]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 96, Loss: 0.007987\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.56 -0.94  2.06]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.06]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.17 -0.86]]\n",
            "b3:\n",
            " [[0.7]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 97, Loss: 0.007935\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.56 -0.94  2.06]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.06]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.17 -0.86]]\n",
            "b3:\n",
            " [[0.7]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 98, Loss: 0.007885\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.56 -0.94  2.06]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.06]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.17 -0.86]]\n",
            "b3:\n",
            " [[0.7]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Epoch 99, Loss: 0.007835\n",
            "Updated Weights and Biases:\n",
            "W1:\n",
            " [[ 1.  2.]\n",
            " [-3.  4.]\n",
            " [ 5.  6.]]\n",
            "b1:\n",
            " [[-1.]\n",
            " [ 1.]\n",
            " [ 2.]]\n",
            "W2:\n",
            " [[ 0.56 -0.94  2.06]\n",
            " [ 0.96  2.96 -2.04]]\n",
            "b2:\n",
            " [[ 0.06]\n",
            " [-1.04]]\n",
            "W3:\n",
            " [[ 2.17 -0.86]]\n",
            "b3:\n",
            " [[0.7]]\n",
            "Final Output (a3): [[0.87]]\n",
            "\n",
            "Final Prediction: [[0.87]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MQanQD2JgZ-H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}